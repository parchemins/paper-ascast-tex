
\section{Adaptive scoped broadcast}
\label{sec:adaptive}

To provide lazy and dynamic logical partitioning in dynamic networks \note{skip: in dynamic networks}
using scoped broadcast, all live \processes must collaborate to
disseminate messages that notify new or removed sources to all and
only interested \processes. This section reviews step-by-step the
properties that allow \processes to converge to the desired state
together. It first defines scoped broadcast, then uses it to guarantee
consistent partitioning when a \process can only become a new source
in the system. It highlights the issue when a \process can also remove
its status of source. It shows that using local knowledge and scoped
broadcast, \processes can still reach dynamic consistent partitioning
when they are able to detect possible blocking conditions in the
dissemination of required notifications. \note{add node apparitions/removals to clearly state what is a dynamic network and because such a removal can also impact the convergence of the system}

\subsection{Scoped broadcast}
\label{subsec:scoped}

In this paper, we consider Edge infrastructures as a set of
interconnected autonomous systems comprising heterogeneous \nodes
interconnected by communication links. \Processes involved in the
management of content may crash but are not byzantine.  \Processes can
reliably communicate through asynchronous message passing to other
known \processes called neighbors.  We define scoped broadcast as a
communication primitive that propagates a message around its
broadcaster within an application-dependant scope.

\begin{definition}[Autonomous system]
  An autonomous system is a network comprising nodes and communication
  links that we represent as a \underline{g}raph of
  \underline{v}ertices and \underline{e}dges:
  $G = \langle V, E \rangle$ with $E \in V \times V$. A
  \underline{p}ath $\pi_{xz}$ from \Process~$x$ to \Process~$z$ is a
  sequence of contiguous edges
  $[\langle x, y_1 \rangle, \langle y_1, y_2\rangle, \ldots, \langle
  y_n, z \rangle]$.
\end{definition}

\begin{definition}[\label{def:scoped}Scoped broadcast]
  When \Process~$x$ scoped \underline{b}roadcasts $b_x(m)$ a
  \underline{m}essage $m$, every correct \process $y$ within a scope
  \underline{r}eceives $r_y(m)$ and \underline{d}elivers it
  $d_y(m)$. The scope depends on the \underline{s}tate $\sigma$ of
  each \process, the \underline{m}etadata $\mu$ piggybacked by each
  message, and a \underline{p}redicate $\phi$ verified from \process
  to \process:
  $(b_x(m) \wedge r_y(m)) \implies \exists \pi_{xy}: \forall z \in
  \pi_{xy}, \phi(\mu_z, \sigma_z)$.
\end{definition}

This definition encompasses more specific definitions of related
work~\cite{hsiao2005scoped, lue2006scoped, wang2015prodiluvian}. It
underlines the transitive relevance of messages. It also highlights
that the functioning of epidemic propagation is well-aligned with the
objectives of scoped broadcast. As consequence, we assume
implementations relying on the forwarding of messages from
neighbor-to-neighbor.

\begin{definition}[\label{def:forwarding}Forwarding]
  When Node $x$ \underline{f}orwards $f_x(m)$ a message $m$, it
  \underline{s}ends $s_{xy}(m)$ the latter to all its neighbors $y$
  \TODO{enriched} ($\oplus$) with metadata that depends on its local
  knowledge $\sigma_x$:
  $f_x(m) \implies \forall \langle x, y\rangle \in E: s_{xy}(m
  \TODO{\oplus} \sigma_x)$.
\end{definition}

We use scoped broadcast to efficiently modify the state of each
\process depending on the partitions that exist in the system.



\subsection{Consistent partitioning}
\label{subsec:consistent}

At any time, a \process can decide to become a \emph{source}, hence
creating a new partition in the system by executing an \texttt{Add}
operation. This partition includes at least its source plus
neighboring \processes that are closer to this source than any other
one. Such a distance (or \emph{weight}) is application-dependant: in
the context of maintaining distributed indexes, this would be about
link latency that \nodes could monitor by aggregating \texttt{ping}s;
or operational costs when dealing with multiple tenants.

\input{input/figadd.tex}

%% \begin{definition}[\label{def:partitioning}Partitioning]
%%   Let $S \subseteq V$ be the set of \underline{s}ources, and $P_{s\in
%%     S}$ be the \underline{p}artition including at least \Process~$s$,
%%   each \process belongs to at most one partition $\forall x, y \in V,
%%   \forall s,s' \in S: (x \in P_{s} \wedge y \in P_{s'}) \implies (x \neq
%%   y \vee s = s')$, and there exists at least one path $\pi_{xs}$ of
%%   \processes that belong to this partition $\forall z \in \pi_{xs}: z
%%   \in P_s$.
%% \end{definition}

%% Definition~\ref{def:scoped} and Definition~\ref{def:partitioning}
%% share the transitive relevance of \process states. However, we further
%% constrain the partitioning in order to guarantee the existence of
%% exactly one consistent partitioning that \processes eventually
%% converge to.

\begin{definition}[\underline{C}onsistent \underline{p}artitioning (CP)]
  Assuming a set of \underline{s}ources $S\subseteq V$, a positive
  \underline{w}eight $w_{xy}$ associated with each edge $\langle x, y
  \rangle \in E$, we define consistent partitioning as a set of
  logical partitions $P_{s\in S}$ where each \node $x$ belongs to the
  partition of its closest source $s$, \ie there exists a
  \underline{p}ath $\pi_{sx}$ with a sum of weights $|\pi_{sx}| =
  \sum\{w_{pq} | \langle p, q \rangle \in \pi_{sx}\}$ smaller than any
  other path, with $|\pi_{xx}|$ being $x$'s greatest lower bound.
  %$\forall x \in
  % P_{s}:  \nexists P_{s'}$ such that $|\pi_{s'x}| < |\pi_{sx}|$.
\end{definition}

Unfortunately, \processes do not share a common global knowledge of
the network state. For \processes to eventually ($\eventually$) reach
consistent partitioning (CP), each source $s$ must send
\texttt{\underline{A}dd} notifications $\alpha_s$ to all \processes
that are closer to it than any other source, along with enough
metadata to allow them to decide of their closest source. Based on
Definition~\ref{def:scoped} and Definition~\ref{def:forwarding}, this
consists in ensuring that each \process $x$ eventually receives
$\alpha_s^{|\Pi_{sx}|}$ where $\Pi_{sx}$ is the \emph{best} path from
any source to it; the best value $|\Pi_{sx}|$ being built over
forwarding along this path.

\begin{theorem}[\label{theo:fb}\underline{F}orwarding of \underline{B}est (FB)
    $\implies\eventually \textup{CP}$]
    % 
    Assuming that each \process $x$ stores its outgoing weights
    ($\forall \langle x, y \rangle \in E: w_{xy} \in \sigma_x$), a
    total order on messages based on weights ($m^d_s < m^{d'}_{s'}$
    when $d < d' \vee (d = d' \wedge s< s')$), reliable communication
    links ($s_{xy}(m) \iff \eventually r_{yx}(m)$), \processes eventually reach
    consistent partitioning if each \process delivers its best (\ie
    smallest) message among \underline{r}eceived messages $R_x$, and
    forwards its best message among \underline{d}elivered messages
    $D_x$ such that $f_x(\alpha^d_s) \implies \forall \langle x, y
    \rangle \in E: s_{xy}(\alpha_s^d \oplus ^{w_{xy}})$, \ie by
    accumulating the respective weight of used edges.
\end{theorem}

\begin{proof}
  Whenever a \process~$s$ becomes a source, it broadcasts hence
  delivers its own message $d_s(\alpha_{s}^{|\pi_{ss}|})$. Whatever
  its set of received messages, it acknowledges that it belongs to its
  own partition $P_s$ since $\alpha_{s}^{|\pi_{ss}|} = \min D_s$ and
  it remains forever since $|\pi_{ss}|$ is its greatest lower bound.
  \noindent Such a source forwards its notification to its neighbors.
  Every neighbor eventually receives its notification since
  communication links are reliable. Whatever the order of received
  messages $R_x$ at neighboring \process~$x$, total order ensures that
  it delivers notifications $\alpha_s^{d}$ when
  $d= |\pi_{ss}| + w_{sx} = \min |\pi_{s'x}|$ where $\min |\pi_{s'x}|$
  is the lightest weight of $x$ to any source $s'$, $s'$ being $s$ in
  this case.
  \noindent Among these neighbors, at least those that fulfill the
  latest condition forward their respective notification.  By
  transitivity, the message originating from $s$ reaches all
  \processes that belong to $P_s$ at least through their respective
  lightest path: $\small\smash{\forall y \in V, s, s' \in S: \min
    |\pi_{s y}| < \min |\pi_{s' y}| \implies \eventually
    d_{y}(\alpha_{s}^{\min |\pi_{sy}|})}$. When the system becomes
  quiescent, \ie no \process becomes source anymore, every \process
  eventually acknowledges the partition it belongs to, \ie \processes
  eventually reach consistent partitioning together. In addition, the
  protocol terminates: a \process never delivers hence forwards a
  message after it received, delivered, and forwarded the message of
  its closest source from its lightest path.
\end{proof}

\begin{algorithm}
  \input{input/algoadd.tex}
  \caption{\label{algo:add}Add-only CP protocol at \Process~$p$.}
\end{algorithm}

Algorithm~\ref{algo:add} shows the instructions that implement a
\emph{reactive} forwarding of best to reach consistent partitioning in
a static network where \processes never join, crash, nor leave the
system. As soon as a \process receives a better message, it delivers
and forwards it. The trade-off consists in decreasing termination time
at cost of increased traffic.  Figure~\ref{fig:add} illustrates the
behavior of this algorithm on a system comprising 4 \processes $a$,
$b$, $c$, and $d$. Both $a$ and $d$ become sources.  They scoped broadcast
notifications $\alpha_a^0$ and $\alpha_d^0$. They initialize their own
state with the lowest value $0$ (see Line~\ref{line:lowestbound}), and
send a message to each of their neighbors by accumulating the
corresponding edge weight (see Line~\ref{line:accumulator}). In
Figure~\ref{fig:addC}, $b$ receives $\alpha_{d}^{1}$. Since it
improves its own partition distance, it keeps it and forwards it to
its neighbors. In Figure~\ref{fig:addD}, $b$ discards
$\alpha_{a}^{2}$, for it does not improve its partition
distance. Neither $c$ nor $d$ will ever acknowledge that Source~$a$
exists.  The protocol lacks of obvious traffic optimization, \eg grey
messages are useless in this scenario. Nevertheless, the system
discards last transiting messages after it reached consistent
partitioning.

Adding logical partitions to a static network is straightforward and
lightweight. Unfortunately, removing partitions or introducing dynamic
network membership increases complexity and costs caused by concurrent
operations.

\subsection{Dynamic consistent partitioning}
\label{subsec:dynamic}

At any time, a source can revoke its status of source by executing a
\texttt{Del} operation, hence deleting its partition from the
system. All \processes that belong to this partition must eventually
choose another partition to belong to.

\input{input/figsimplerproblem.tex} %% positioning

\input{input/figdel.tex}

\begin{definition}[\label{def:dcp}\underline{D}ynamic
    \underline{c}onsistent \underline{p}artitioning (DCP)] A DCP
  protocol enables \processes to join or leave the set of sources at
  any time while ensuring eventual consistent partitioning.
  % Dynamic consistent partitioning is consistent partitioning where
  % \processes can join or leave the set of sources at any \TODO{time}.
\end{definition}

Delete operations bring a new notion of order between events, and most
importantly between message deliveries. A \process $s$ that performs a
\texttt{Del} operation implicitly states that its preceding
\texttt{Add} operation becomes obsolete. As consequence, their results
-- scoped broadcast in the form of \texttt{\underline{A}dd}
notifications $\alpha_s$ -- should be canceled by the corresponding
\texttt{\underline{D}el} notifications of staleness $\delta_s$. We
focus on the last delivery of each \process, since the best is also
the last, as highlighted by Algorithm~\ref{algo:add}.

%% \TODO{More about useless $\alpha$ ?}

\begin{definition}[Happens-before ($\rightarrow$)~\cite{lamport1978time}]
  The transitive, irreflexive, and antisymmetric happens-before
  relationship defines a strict partial order between events.  Two
  messages are concurrent if none happens before the other.
\end{definition}

\begin{definition}[\label{def:lwo}Stale messages]
  Only the latest broadcast of a \node matters.  A message $m$ conveys
  \underline{s}tale control information $\mathcal{S}(m)$ as soon as
  its broadcaster broadcasts another message: $\mathcal{S}(\alpha_x) =
  \exists \delta_x: b_x(\alpha_x) \rightarrow b_x(\delta_x)$.  A
  \process only delivers or sends messages that it assumes up-to-date.
  % : $s_x(\alpha_s)
  % \implies \delta_s \not\in R_x$.
  For convenience, we define
  % \TODO{$\textup{minUTD(}D\textup{)} = \min
  %  \{\alpha_s^d | \alpha_s^d \in D \wedge \delta_s \notin D\}$}, and
  $\textup{last(}D\textup{)} = \alpha_s \in D: \nexists \alpha_{s'}
  \in D: d_x(\alpha_s) \rightarrow d_x(\alpha_{s'})$.
  %  \TODO{Only keep last?  Tothink what happens if we keep all as in
%    def now.}
\end{definition}

\newcommand{\last}{\textup{last}~}

A naive attempt at implementing DCP resembles echoes in acoustics: a
sound propagates in the air before crashing into surrounding walls to
finally come back altered. Following the principles of scoped
broadcast as stated in Definition~\ref{def:scoped}, a \process $x$
that receives a staleness notification $\delta_s$ forwards it only if
the latter targets its current partition $\alpha_s$. The staleness
notifications propagate through the network as long as they remain in
the deleted partition. When they reach the bordering \processes of the
deleted partition, it creates an echo of bordering partitions that
will go backward to fill the gap left open by removals using
forwarding of best (FB) as stated by Theorem~\ref{theo:fb}.

\input{input/figundoproblem.tex}

\begin{definition}[\label{def:fs}\underline{F}orwarding of
  \underline{s}taleness (FS)]
  %
  Any source can broadcast a staleness notification at any time. Every
  \process $x$ delivers and forwards a received staleness notification
  $\delta_s$ if it targets its best up-to-date delivered message:
  %% \TODO{$(\delta_s \in R_x \wedge \alpha_s =
  %%     \textup{minUTD}~D_x) \iff \eventually (d_x(\delta_s) \rightarrow
  %% f_x(\delta_s))$.}
  $(\last D_x = \alpha_s \wedge d_x(\alpha_s) \rightarrow
  r_x(\delta_s)) \implies d_x(\delta_s) \rightarrow f_x(\delta_s)$.
  % \wedge \nexists d'_s: d_x(\delta_s) \TODO{\not\rightarrow} d_x(\delta_s'))$.
\end{definition}

\begin{definition}[\label{def:fbplus}$\textup{FB}^+$: echos]
  In addition to FB, a \process $x$ that receives but does not deliver
  a staleness notification $\delta_{s}$ sends back --~or echoes~-- its
  best up-to-date delivered message: $(\last D_x = \alpha_{s'}^d
  \wedge \delta_{s'} \not\in D_x \wedge d_x(\alpha_{s'}^d)\rightarrow
  r_{xy}(\delta_{s})) \implies r_{xy}(\delta_{s}) \rightarrow
  s_{xy}(\alpha_{s'}^d\oplus^{w_{xy}})$.

  
  % $r_{xy}(\alpha_s)
  % \wedge \delta_s \in D_x \implies s_{xy}(\alpha_{s'}^d\oplus
  % ^{w_{xy}})$ with $\alpha_{s'^d} = \min D_x \wedge \delta_{s'} \not\in
  % D_x$. \TODO{Rework min}
\end{definition}

Figure~\ref{fig:del} illustrates the behavior of this implementation
($\textup{FB}^+ \wedge \textup{FS}$).  Two partitions initially exist:
$P_a$ and $P_d$ that respectively include $\{a\}$, and $\{b, c,
d\}$. In Figure~\ref{fig:delA}, $a$ deletes its partition. FS in
Definition~\ref{def:fs} states that $a$ must notify all its
neighbors~--~here only $b$~-- that may belong to its partition.  In
Figure~\ref{fig:delB}, $b$ receives but does not deliver $\delta_a$
since $\delta_a$ does not target its current partition $P_d$.
$\textup{FB}^+$ in Definition~\ref{def:fbplus} states that $b$ must
echo back its own best up-to-date message $\alpha_d^3$, for it may be
the best for $a$. Eventually, every \process belongs to Partition
$P_d$. In this scenario, they reach consistent
partitioning. Unfortunately, this protocol does not ensure consistent
partitioning in every scenario.

% \TODO{The naive implementation described above is $\textup{FB}^+
%  \wedge FS$. And it does  not work.}

\begin{lemma}[\label{lem:fbfse}$\textup{FB}^+ \wedge \textup{FS}
    \centernot\implies \textup{DCP}$] Forwarding best up-to-date,
  forwarding staleness, and echoing is not sufficient to guarantee
  dynamic consistent partitioning.
\end{lemma}

\begin{proof}
Stale control information (see Definition~\ref{def:lwo}) may impair
the propagation of both
\begin{inparaenum}[(i)]
\item notifications about actual sources, and
\item notifications about deleted partitions.
\end{inparaenum}
Figure~\ref{fig:problem} depicts a scenario comprising only three
\processes $a$, $b$, $c$ chained with FIFO links, \ie where \processes
receive the messages in the order of their sending ($s_{pq}(m)
\rightarrow s_{pq}(m') \implies r_q(m) \rightarrow r_q(m')$). In
Figure~\ref{fig:problemA}, both $a$ and $c$ become sources, sending
their respective notification message to $b$. In
Figure~\ref{fig:problemB}, both $a$ and $c$ delete their partition
while $b$ receives, delivers, and forwards $\alpha_a^2$. In
Figure~\ref{fig:problemC}, $b$ receives, delivers, and forwards
$\alpha_c^1$, for it improves its best partition. Then it receives and
discards $\delta_a$, for its best partition does not match the deleted
one. It echoes back $\alpha_c^3$ to $a$. In Figures~\ref{fig:problemD}
and \ref{fig:problemE}, $a$ and $b$ handle transiting
notifications. Finally, Figure~\ref{fig:problemF} shows that
eventually, $c$ stays in the deleted partition $P_a$ for not having
received $\delta_a$ that $b$ blocked earlier. The system does not
reach consistent partitioning. \end{proof}

The issue is that each \process trusts its parent to forward all
staleness notifications relevant to it. When this fails, as in
Figure~\ref{fig:problem}, not only a \process ($c$) may wrongfully
stay in a partition ($P_a$) when its source ($a$) already deleted it,
but it may contribute to its inconsistency by not forwarding farther
but up-to-date messages from live sources.

A first step to avoid staying in inconsistent state is to extend the
behavior of each \process so \processes such as $c$, that would remain
in a wrong partition, use their history of receipt and delivery to
detect the possible blocking of staleness notifications that can
hinder reaching consistent partitioning.  In Figure~\ref{fig:problem},
$b$ blocked the staleness notification $\delta_a$ that $c$ needs since
it belongs to a ephemeral partition $P_c$ that $c$ acknowledges to be
stale before $b$ does. Therefore, $c$ can detect the possible blocking
of $\delta_a$ since it acknowledges that its parent $b$ received,
delivered, and forwarded the stale notification $\alpha_c^1$. In other
terms, $c$ acknowledges that between the delivery of $\alpha_c^1$ and
the delivery of $\delta_c$, $b$ blocked any other $\delta$ and
therefore, it could have blocked $\delta_a$.

%% \Begincomme{inparaenum}[(i)] \item to detect possibly blocking
%% conditions that can hinder \item to purge all stale messages from
%% the system \item enabling up-to-date notifications to eventually
%% echo back to every \process.  \end{inparaenum} Since
%% Figure~\ref{fig:problemD} highlights that when the protocol
%% terminates, $c$ is the \process that remains in the wrong
%% partition, we put the burden of detection and resolution on it.

%% \TODO{A lot of blockings occur, only that of example is a problem.
%%   Could be $b$ or $c$ that detects? We put burden on $c$ since
%%   unconsistency starts here. Problem: how to make sure that $c$
%%   receives a staleness notification in this case? Also, $b$ is not
%%   sure to receive $\delta_c$ since composable problem.}

\begin{lemma}[\label{lem:detector}$\textup{FB}^+ \wedge \textup{FS}
  \implies$ Detector existence] Assuming
  $\textup{FB}^+$ and $\textup{FS}$, a chain of delivery on $\pi_{xz}$ of
  $\alpha_x$ with $\alpha_x = \last D_z$, if $x$ broadcasts a staleness
  notification $\delta_x$, either
  \begin{inparaenum}[(A)]
  \item\label{lem:detectorA}  $z$ eventually delivers it, or 
  \item\label{lem:detectorB} a \process $y$ in such a chain of
    delivery $\pi_{xz}'$ eventually detects the possible blocking of $\delta_x$.
  \end{inparaenum}
  % $b_p(\delta_x) \implies d_q(x) U Detect[X]$ ??? 
\end{lemma}


\begin{proof}
  %% knowing that b_p delta is sufficient to say that processes will
  %% receive delta or detect blocking
  Assuming a chain of delivery $\pi_{xz} = [x, \ldots, z]$ of
  $\alpha_x$ with $\alpha_x = \last D_z$ and $d_x(\delta_x$), we must
  prove that whatever the possible states of \nodes that belong to
  this chain, it either leads to outcome (\ref{lem:detectorA}) or
  (\ref{lem:detectorB}).
  \begin{asparadesc}
  \item [(I) Same last partition:] $\forall y \in \pi_{xz}: \alpha_x =
    \last D_y$, with forwarding of staleness
    (Definition~\ref{def:fs}), $f_{\pi_{xz}[k]}(\delta_x) \implies
    \eventually d_{\pi_{xz}[k+1]}(\delta_x)$ except if $\delta_x \in
    D_{\pi_{xz}[k+1]}$ which means that $\pi_{xz}[k+1]$ already
    delivered and forwarded $\delta_x$ following another such a chain
    of delivery. Therefore, $z$ eventually receives, hence delivers
    $\delta_x$ (outcome \ref{lem:detectorA}).
    
  \item[(II) Different last partitions:] $\exists y \in \pi_{xz}
    \setminus \{x, z\}: \alpha_s = \last D_y$, by (I), the staleness
    notification reaches the first of such a \process $y =
    \pi_{xz}[k]$.  Forwarding of staleness (Definition~\ref{def:fs})
    states that the forwarding stops when $y$ already delivered
    $\delta_x$ ($\delta_x \in D_y$ covered by (I)) or delivered a
    better message $\alpha_s = \last D_y$.  With $y'=\pi_{xz}[k+1]$,
    forwarding of best (Definition~\ref{def:fbplus}) implies three
    possible results:
  \begin{asparadesc}
  \item [(i) $y'$ equivalent to $y$:] $\alpha_{s'} = \last D_{y'}$
    with $\alpha_{s'} \neq \alpha_x$ hence $y' \iff y$ which leaves
    two possible results as follows:
  \item [(ii) $y'$ in $P_x$ from another parent:] $r_{y'}(\alpha_s)
    \wedge \alpha_x = \last D_{y'} \wedge \alpha_x < \alpha_s$ which
    means that a shorter path of delivery $\pi_{xz}'$ exists that
    either forwards appropriate staleness notifications (covered by
    (I)) or detects its possible blocking as follows:
  \item [(iii) $y'$ in $P_x$ with $y$ as parent] but does not deliver
    the $y$'s last partition for it already delivered the
    corresponding staleness notification: $r_{y'}(\alpha_s) \wedge
    \delta_s \in D_{y'} \wedge \alpha_x = \last D_{y'}$ which detects
    a possible blocking of $\delta_x$ (outcome
    (\ref{lem:detectorB})). Without global knowledge, $y'$ assumes it
    belongs to the shortest and only path of delivery of $\alpha_x$
    % ($\pi_{xy'} \subseteq \Pi_{xz}$)
    thus it cannot further delegate the detection to another \process.
  \end{asparadesc}
  \end{asparadesc}
  \vspace{-1.5em} %% ugly vspace but necessary for enumerate+qed
  %
  %% (i) If every \process (such as $a$) starting from the source
  %% delivers and forwards a removal notification $\delta_X$ when their
  %% last delivery is $\alpha_X$, then every such \process eventually
  %% delivers the removal notification $\delta_X$ except \processes (such
  %% as $b$) that delivered $\alpha_X$ from a \process that delivered a
  %% message from another partition $\alpha_Y$ since then.
  %
  %% These exceptions eventually receive and deliver $\alpha_Y$ since
  %% $\alpha_Y$ is better than $\alpha_X$ through this path, except if
  %% they already received and delivered the removal notification of
  %% $P_Y$ (such as $c$). These \processes may never receive hence
  %% deliver $\delta_X$, and may never receive hence deliver a better
  %% message than $\alpha_X$. They need an additional mechanism to
  %% eventually purge $\alpha_X$ that cannot rely on the eventual purging
  %% of $P_Y$ at \processes like $b$, to avoid deadlocks.
  %
  %% (ii) Assuming that every \process keeps an history of its past
  %% deliveries, \processes (such as $c$) can detect the inconsistency,
  %% since they receive from their parent an already deleted partition
  %% $P_Y$. This notification means that the parent discards any
  %% $\delta_Z$ with $\alpha_Z^z < \alpha_Y^y$, and most importantly, if
  %% $\delta_X$ exists, it discards it. To ensure the purge of stale
  %% messages, a detecting \process must assume the worst case that such
  %% $\delta_X$ exists, and send another kind of message, noted $\Delta$,
  %% that notify the possible removal of $P_X$.
  %
  %% (iii) A \process (such as $d$) whose last delivery is $\alpha_X$,
  %% but whose parent is neither inconsistent (like $b$) nor receiving
  %% $\delta_X$ (like $a$), eventually receives $\Delta$ from a detecting
  %% \process, either directly or transitively, for such a child \process
  %% ($d$) trusts the possible removal of $P_X$ by delivering and
  %% forwarding such $\Delta$. $\Delta$ suffers from identical blocking
  %% conditions (between $b$ and $c$) than $\delta$, leading to the same
  %% solutions of detection and propagation of $\Delta$. Eventually,
  %% every \process whose last delivery is $\alpha_X$ receives and
  %% delivers either a better $\alpha_Z$, or $\delta_X$, or $\Delta_X$.
\end{proof}

As a second step, detecting \processes must proactively purge the
system from their forwarded notifications. For instance in
Figure~\ref{fig:problemF}, $c$ detects the blocking of the staleness
notification $\delta_a$. It could broadcast $\delta_a$ in order to
acknowledge the staleness of its own $\alpha_a^3$ and inform
neighboring \processes that delivered it as well. Unfortunately, there
exists false positive in the blocking detection.
Lemma~\ref{lem:detector}'s proof shows that the detection at $y'$ does
not depend on $\delta_x$. Therefore $\delta_x$ may not exist but $y'$
still receives $\alpha_s$ after the delivery of $\delta_s$ from the
path it received and delivered
$\alpha_x$. Figure~\ref{fig:undoproblem} highlights this behavior. In
Figure~\ref{fig:undoproblemB}, $a$ does not delete its partition. In
Figure~\ref{fig:undoproblemF} and because of scoped broadcast, $c$
received the same series of messages as in
Figure~\ref{fig:problemF}. Yet, $c$ must assume the existence of
$\delta_a$ and act accordingly by forcing the staleness of its best
delivered message and disseminate this information to its neighbors.
To avoid flooding the system with false positive staleness
notifications, we reduce the scope of staleness notifications by
including only downstream \processes. In
Figure~\ref{fig:undoproblemF}, false positives would generate traffic
at $c$ and its children if it had any, without impacting $a$ and $b$.

\begin{definition}[\label{def:fsplus}$\textup{FS}^+$: forwarding
    staleness downstream] Any \emph{\process} can broadcast a
  staleness notification at any time. A \emph{child \process} $x$
  delivers and forwards a received staleness notification if it comes
  from the path of delivery of its best up-to-date delivered
  message.
\end{definition}

It is worth noting that forwarding of staleness as stated in
Definition~\ref{def:fs} becomes a specific case of
Definition~\ref{def:fsplus} where the source itself forwards a
staleness notification downstream. The notification must reach all
\processes in its partition since the source has no \processes
upstream, and belongs to the delivery path of all \processes that
delivered this message.

\begin{theorem}[$\textup{FB}^+ \wedge \textup{FS}^+ \implies \textup{DCP}$]
  A protocol guarantees dynamic consistent partitioning if it
  implements forwarding of best up-to-date messages with echos,
  forwarding of staleness messages downstream, and the detection of
  possible blocking triggers the forwarding of staleness notifications
  downstream.
\end{theorem}

\begin{proof}
  Detection triggers forwarding of staleness downstream which
  completes the case study of Lemma~\ref{lem:detector} by ensuring
  that, when a source broadcasts its staleness notification, all
  \processes that belong to its partition eventually deliver such a
  staleness notification. All downstream bordering \processes also
  eventually receive such a staleness notification and echo back their
  best delivered message. This triggers another competition as in
  Theorem~\ref{theo:fb} for the \processes that delivered the
  staleness notification.
\end{proof}

\begin{algorithm}
  \input{input/algoascast.tex}
  \caption{\label{algo:ascast}\NAME: DCP protocol at \Process~$p$.}
\end{algorithm}

Algorithm~\ref{algo:ascast} shows the instructions of our
implementation called \NAME that enables dynamic consistent
partitioning. It implements forwarding of best (see
Line~\ref{line:fb}) and echos (see Line~\ref{line:echo}). To implement
forwarding of staleness downstream as stated in
Definition~\ref{def:fsplus}:
\begin{inparaenum}[(A)]
\item each \process maintains a vector of versions that associates the
  respective known local counter of each known source, or has-been
  source. It constitutes a summary of known progress of other
  \processes; 
\item each notification message $\alpha$ carries the list of
  identifier and counter of each \node that forwarded it. In the worst
  case, both these structures grow linearly with the number of
  \processes in the system $\mathcal{O}(V)$. Nevertheless, following
  the principles of scoped broadcast, we expect that \processes only
  acknowledge a small subset of sources and messages;
\item each staleness notifications $\delta$ only carry the identifier
  and counter of the \process~--~source or detector~--~that generated
  it. Only downstream \processes may deliver such message, since they
  carry the identifier of the generator. 
\end{inparaenum}
To implement the detection as stated in Lemma~\ref{lem:detector}, each
\process only requires to know the direct parent of its best delivered
message which is already included in the piggybacked path of this
message. Receiving a message known to be stale from this parent
triggers the generation of staleness notifications that can only be
delivered by downstream \processes (see Line~\ref{line:detect}).

\begin{algorithm}[h]
  \input{input/algoedges.tex}
  \caption{\label{algo:edges}\NAME at \Process~$p$ in dynamic networks.}
\end{algorithm}

By reusing \NAME's default behavior of echos and downstream staleness,
Algorithm~\ref{algo:edges} enables dynamic consistent partitioning
even in dynamic networks subject to physical partitioning where
\processes can join, leave, or crash at any time. Adding a \process is
equivalent to add as many edges as necessary. Since adding an edge may
improve shortest paths, the protocol triggers a competition using
echos of Definition~\ref{def:fbplus}.  Removing a \process is
equivalent to removing all its edges. Removing an edge between two
\processes may invalidate the shortest path of one of involved
\processes plus downstream \processes, or impair the propagation of
staleness notifications. Therefore, the protocol reuses its detectors
of Lemma~\ref{lem:detector} to remove irrelevant messages from the
system.



%% \subsection{Lazy dynamic consistent partitioning}
%% \label{subsec:lazy}

%% \input{input/figxasproblem.tex}

%% To further reduce the traffic generated by our DCP protocol, we
%% leverage that an Edge infrastructure is a \TODO{network of autonomous
%% systems}, only a small subset of which needs logical partitioning.

%% \begin{definition}[Edge infrastructure]
%%   An Edge infrastructure $\mathcal{G}$ has multiple autonomous systems
%%   interconnected with additional links $\mathcal{E} \in V_1 \times
%%   V_2$ where $V_1 \neq V_2$, hence $\mathcal{G} = \langle \{G_1,
%%   \ldots G_n\}, \mathcal{E} \rangle$. A \underline{g}ateway $g$ is a
%%   \process with at least one link towards another network: $\langle g,
%%   \_ \rangle \in \mathcal{E}$.
%% \end{definition}

%% At any time, an autonomous system start logical partitioning when a
%% \process from this network becomes a source.  Then every \process from
%% this network belongs to the partition of its closest source not only
%% from their own network but that of adjacent networks that started
%% logical partitioning as well.  At any time, an autonomous system stops
%% logical partitioning when the last source from this network revokes
%% its status of source. Eventually, all nodes from this network do not
%% belong to any partition and the generated traffic stops.

%% \begin{definition}[\TODO{Adjacent networks}]
%% \end{definition}

%% \begin{definition}[Lazy dynamic consistent partitioning (LDCP)]
%%   \TODO{If there exists at least a source in a network, its nodes must
%%     belong to the partition of the closest partition in adjacent
%%     networks. Otherwise, nodes must eventually belong to no
%%     partition.}
%% \end{definition}

%% \TODO{A naive implementation would consist in using gateways as
%%   additional gatekeepers of scoped broadcast. Figure that shows that
%%   it works mostly.}

%% \begin{lemma}[$\textup{FB}^+ \wedge \textup{FS}^+ \wedge \textup{G} \centernot\implies \textup{LDCP}$]
%%   \TODO{meow}
%% \end{lemma}

%% \begin{proof}
%%   \TODO{meow meow}
%% \end{proof}

%% \TODO{To enable LDCP in an Edge infrastructure, we use the property provided
%% by DCP in an autonomous system to all its \processes and among others,
%% to its gateways.}

%% \begin{lemma}[Source existence]
%%   When a system becomes quiescent, DCP guarantees that every \process
%%   eventually acknowledges the existence or non-existence of a source
%%   in its system.
%% \end{lemma}

%% \begin{proof}
%%   By design.
%% \end{proof}

%% The gateways become responsible for the flooding or the stopping of
%% notifications foreign to their respective network. They must
%% differentiate between local and global notifications. Local
%% notifications provide both \begin{inparaenum}[(i)]
%% \item local source existence, and
%% \item closest local source in the network
%% \end{inparaenum}; while global notifications only improve on closest global sources,
%% \ie it does not need to propagate when the foreign source is farther
%% than the local one.

%% \begin{definition}[\underline{B}roadcasting of \underline{u}ninterested (BU)]
%%   \TODO{meow}
%% \end{definition} 

%% \TODO{Blocking condition local global remains.}

%% \TODO{Added invariant: global is always smaller than local when they
%%   exist.}

%% \begin{algorithm}[h]
%%   \input{input/algoxascast.tex}
%%   \caption{\label{algo:xascast}\NAMEC at \Process~$p$. \TODO{TODO.}}
%% \end{algorithm}

\NAME follows the principles of scoped broadcast to efficiently and
consistently propagate notifications to \processes that need them
despite concurrent operations. Next Section aims at providing the
empirical evidence of scalability and properties of the proposed
approach.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 
