%% to better adapt positioning (I expect 2.5 pages
%% intro+related work)

% \vfill \TODO{spacing for intro and related work: 2.5 pages}
% \newpage


\section{Adaptive scoped broadcast}
\label{sec:adaptive}

%% In this section, we propose to dynamically adapt the scopes of \NAMEB
%% using \NAMEB itself for the sake of efficiency: a \NAMEB message may
%% modify the state of a \process, consequently changing predicate
%% evaluations over deliveries.  At any time, a \process decides whether
%% or not it becomes the center (or \emph{source}) of a scope (or
%% \emph{partition}). \Processes must eventually rally the partition
%% corresponding to their closest source despite concurrent operations,
%% and dynamic changes in the network.

%% \noindent In that regard, we introduce \NAME (stands for
%% \underline{A}daptive \underline{S}coped broad\underline{cast}), a
%% wait-free reactive protocol for dynamic logical partitioning that
%% enables adaptive scoped broadcast in dynamic distributed
%% systems. \NAME's operation guarantees that \processes eventually reach
%% consistent partitioning by efficiently propagating their state
%% changes. \NAME's overhead actually depends on its operations and
%% current partitions in the system. When the system becomes quiescent,
%% \processes eventually converge to their respective partition and do not
%% require further communication afterward.

%% \noindent This section starts by introducing consistent partitioning
%% and its implementation when a \process can only become a new source to
%% create a new partition, i.e., it cannot revoke its self-appointed
%% status of source. Then, it extends to include the latter behavior: it
%% introduces the properties that guarantee consistent partitioning in
%% such context, and it details our implementation called \NAME along
%% with its complexity analysis.

To assign and maintain \processes to their best partition according to
replica creations/removals as well as dynamic infrastructure changes,
we designed and implemented \NAME (\NAME stands for
\underline{A}daptive \underline{S}coped broad\underline{cast}).  \NAME
relies on a primitive that allows a \process to broadcast a message
within a limited scope.  This primitive is used in a first algorithm
to deliver consistent partitioning each time \process creates a new
replica within the system. This algorithm is then extended to include
replica removals as well as dynamic changes of the infrastructure.
This section discussed the different properties that guarantee
consistent partitioning in such a context and details the \NAME
implementation along with its complexity analysis.


\subsection{Scoped broadcast}
\label{subsec:scoped}

%For instance, a \process from Paris could scoped broadcast messages to
%all \processes in Paris; and \processes from other cities would never
%deliver such messages.
%Scoped broadcast only targets a connected \emph{subset} of \processes
%in the whole distributed system.
%

% \TODO{dynamic graph: superscript with $t$?}
In the edge infrastructure we consider, nodes are heterogeneous and
interconnected by communication links. \Processes involved in the
management of the data sets may crash but are not byzantine.  Finally,
\processes can reliably communicate through asynchronous message
passing to other known \processes called neighbors.
A more formal definition of our system is given below:

\begin{definition}[Edge infrastructure]
  An edge infrastructure is a connected \underline{g}raph $G(V, E)$ of
  \underline{v}ertices $V$ and bidirectional \underline{e}dges $E
  \subseteq V \times V$.  A \underline{p}ath $\pi_{ij}$ from
  \Process~$i$ to \Process~$j$ is a sequence of vertices $[i, k_1,
    k_2, \ldots k_n, j]$ with $\forall m: 0\leq m \leq n, \langle
  \pi_{ij}[m], \pi_{ij}[m+1] \rangle \in E$.
\end{definition}


%Using epidemic
%propagation (or gossiping), \processes can reliably broadcast messages
%by forwarding delivered messages from neighbors to
%neighbors~\cite{birman1999bimodal, hadzilacos1994modular,
%  nedelec2018causal, raynal2013distributed}.
%
%However, uniform reliable
%broadcast states that \emph{all} correct \processes must deliver all
%broadcast messages. This proves costly in large scale systems
%comprising thousands of \processes. Instead, we define \emph{scoped
%broadcast} where messages reach only an application-dependant subset
%of connected \processes, thus significantly reducing the generated
%traffic of broadcast.

We define the Scoped broadcast (\NAMEB) as a communication primitive that propagates a
message around its broadcaster within an application-dependant scope :


\begin{definition}[\label{def:scoped}\underline{S}coped broad\underline{cast} (\NAMEB)]
  When \Process~$x$ scoped \underline{b}roadcasts $b_x(m)$ a
  \underline{m}essage $m$, every correct \process $y$ within a scope
  \underline{r}eceives $r_y(m)$ and \underline{d}elivers it
  $d_y(m)$. The scope depends on the \underline{s}tate $\sigma$ of
  each \process, the \underline{m}etadata $\mu$ piggybacked by each
  message, and a \underline{p}redicate $\phi$ verified from \process to
  \process: {\small{$b_x(m) \wedge r_y(m) \implies \exists \pi_{xy}: \forall z
  \in \pi_{xy}, \phi(\mu_z, \sigma_z)$}}.
\end{definition}

This definition encompasses more specific definitions of related
work~\cite{hsiao2005scoped, lue2006scoped, wang2015prodiluvian}.  It
also highlights that epidemic propagation and scoped broadcast have
well-aligned preoccupations. More precisely, it underlines the transitive relevance of
messages, thus \processes can stop forwarding messages as soon as the
corresponding predicate becomes unverified.
%For instance, a \process
%from Paris could scoped broadcast messages to all \processes in
%Paris. This requires \processes to store and maintain their city
%location in their local state. \Processes stop delivering and
%forwarding their received messages when they come from a different
%city.  In another instance, a \process from Paris could scoped
%broadcast messages to all \processes in Paris plus neighboring
%cities. This requires \processes to overload forwarded messages the
%first time they reach another city. The predicate checks if messages
%already reached two distinct cities before delivery. Similarly to
%uniform reliable broadcast, scoped broadcast implementations expose
%different trade-offs on space, time, and communication.

\NAMEB is used to change the state of each \process dynamically, depending on all partitions that exist in the system.



\subsection{Consistent partitioning}

At any time, a \process can decide to become a source, hence creating
a new partition in the system by executing an \texttt{Add}
operation. This partition includes at least its source plus
neighboring \processes that estimate they are closer to this source
than any other one. Such distance (or \emph{weight}) is
application-dependant: in the context of maintaining distributed
indexes, weights would be link latency.

\begin{definition}[\label{def:partitioning}Partitioning]
  Let $S \subseteq V$ be the set of \underline{s}ources, and $P_s$ be
  the \underline{p}artition including at least \Process~$s$, each
  \process belongs to at most one partition \smash{\small$\forall p,q \in V, \forall
  s_1,s_2 \in S: p \in P_{s_1} \wedge q \in P_{s_2} \implies p \neq q
  \vee s_1 = s_2$}, and there exists at least one path $\pi_{ps}$ of
  \processes that belong to this partition $\forall q \in \pi_{ps}: q
  \in P_s$.
\end{definition}

Definition~\ref{def:scoped} and Definition~\ref{def:partitioning}
share the transitive relevance of \process states. However, we further
constrain the partitioning in order to guarantee the existence of
exactly one consistent partitioning that \processes eventually converge
to.

\begin{definition}[\underline{C}onsistent \underline{p}artitioning (CP)]
  Let $W_{xy} = W_{yx}$ be the positive symmetric \underline{w}eight
  between $x$ and $y$, $\Pi_{xz}$ be the shortest \underline{p}ath
  from $x$ to $z$ the weight of which $|\Pi_{xz}|$ is lower than any
  other path weight, the only consistent partitioning $\mathcal{P}$ is
  a set of partitions $P_{s\in S}$ such that each \process belongs to
  a logical partition comprising its closest source: $\forall p \in
  P_{s_1}, \nexists P_{s_2}$ such that $|\Pi_{s_1p}| > |\Pi_{s_2p}|$.
\end{definition}

Unfortunately, \processes do not share a common global knowledge of
the network state. For \processes to reach consistent partitioning
together, a \process becoming a source must notify every \process that
is closer to it than any other source. Since epidemic dissemination
and scoped broadcast have well-aligned preoccupations, we assume
implementations relying on forwarding of messages from neighbor to
neighbor.

\begin{theorem}[\label{theo:bef}\underline{B}est \underline{e}ventual
    \underline{f}orwarding $(BEF) \implies CP$]
%
Assuming reliable communication links where a correct \process $q$
eventually receives the message $m$ \underline{s}ent to it by a
\process $p$ ($s_{pq}(m) \implies r_{q}(m)$), \processes eventually
reach consistent partitioning if each \process eventually forwards its
best known partition.
\end{theorem}

\input{input/figadd.tex} 

\begin{proof}
  % \begin{asparadesc}
  % \item [$BEF \implies CP$:]
  When a \process $s_1$ becomes a source, it belongs to its own
  partition, for there exists no better partition than its own:
  $\forall p \in V: |\Pi_{s_1 s_1}| < |\Pi_{s_1 p}|$. It delivers,
  hence forwards such $\alpha$ message to its neighbors. Since
  communication links are reliable, neighboring \processes eventually
  receive such notification $\forall \langle s_1, q \rangle \in E, s_1
  \in S \iff \eventually r_q(\alpha_{s_1}^{w_{s_1 q}})$. Most
  importantly, whatever the order of received messages, every \process
  $q'$ in this neighborhood -- such that there exists no better
  partition $s_2$ than the received one -- delivers and forwards it:
  $\forall s_2 \in S: |\Pi_{q' s_1}| < |\Pi_{q' s_2}| \implies
  \eventually d_{q'}(\alpha_{s_1}^{w_{s_1 q'}})$. By transitivity, the
  message originating from $s_1$ reaches all such \processes through
  their shortest paths: $\forall q'' \in V, s_1, s_2 \in S: |\Pi_{q''
    s_1}| < |\Pi_{q'' s_2}| \implies \eventually
  d_{q''}(\alpha_{s_1}^{|\Pi_{s_1 q''}|})$.  Since there exists only
  one best sum of weights per \process that can never be retracted,
  \processes eventually reach consistent partitioning.
  %% /!\ not equivalence for there exists other implementations.
  %% \item [$CP \implies BEF$:] By contradiction, if a \process $q \in
  %%   \Pi_{sp} = [s, \ldots, q, q', \ldots, p]$ with $s, q, p \in P_s$
  %%   does not forward its received $\alpha_s^{|\Pi_{sq}|}$, then
  %%   following \processes from $q'$ to $p$ may mistake another
  %%   partition for their  because it needs the weight $W_{pq}$.
  %% \end{asparadesc}
\end{proof}

\begin{algorithm}
  \input{input/algoadd.tex}
  \caption{\label{algo:add}Adding a partition by \Process~$p$.}
\end{algorithm}

Algorithm~\ref{algo:add} shows the instructions that implement such
consistent partitioning when
\begin{inparaenum}[(i)]
\item weights are scalar values,
\item \processes only add new partitions to the system,
\item and \processes never crash nor leave the system.
\end{inparaenum}
Figure~\ref{fig:add} illustrates its behavior on a system comprising 4
\processes $a$, $b$, $c$, and $d$. \Process~$a$ and \Process~$d$
become the sources of their partition. They \NAMEB a notification
\underline{a}dd message: $\alpha_a^0$ and $\alpha_d^0$. They
initialize their own state with the lowest possible bound $0$ (see
Line~\ref{line:lowestbound}), and send a message to each of their
neighbors by accumulating the corresponding edge weight (see
Line~\ref{line:accumulator}). In Figure~\ref{fig:addC}, \Process~$b$
receives $\alpha_{d}^{1}$. Since it improves its own partition
distance, it keeps it and forwards it to its neighbors. In
Figure~\ref{fig:addD}, \Process~$b$ discards $\alpha_{a}^{2}$, for it
does not improve its partition distance. \Processes $c$ and $d$ will
never acknowledge that Source~$a$ exists. Ultimately, \processes
discard last transiting messages. Despite the obvious lack of traffic
optimization, the system reach consistent partitioning.

While only adding logical partitions to the distributed system is
straightforward, removing them introduces additional complexity.

\subsection{Dynamic consistent partitioning}
\label{subsec:dynamic}

%% At any time, a \process can become a source, hence adding a new
%% partition to the system. This partition eventually includes all
%% \processes that are closer from this new source than any other
%% else. \Processes naturally converge towards their respective best
%% partition by only piggybacking a monotonically increasing distance in
%% forwarded messages.

At any time, a source can revoke its self-appointed status of source
by executing a \texttt{Del} operation, hence deleting its partition
from the system. All \processes that belong to this partition must
eventually choose another partition to belong to. In
Figure~\ref{fig:del}, two partitions exist initially: $P_a$ and $P_d$
that respectively include $\{a\}$, and $\{b, c, d\}$. In
Figure~\ref{fig:delA}, \Process~$a$ deletes its partition. It notifies
all neighboring \processes -- here only \Process~$b$ -- that may
belong to its partition using \NAMEB. Upon receipt, \Process~$b$
discards the message $\delta_a$, for the latter does not target the
former's partition. \Process~$b$ sends its own knowledge of best
partition $\alpha_d^3$ that may prove to be the best for
\Process~$a$. Eventually, every \processes belong to the same
partition $P_d$. In this scenario, they reach consistent partitioning.

Delete operations add a new notion of order between events, and most
importantly between message deliveries. Delete operations implicitly
state that all preceding events become obsolete, and that all messages
originating from these preceding events convey stale control
information.

\begin{definition}[Happens-before $\rightarrow$~\cite{lamport1978time}]
  The transitive, irreflexive, and antisymmetric happens-before
  relationship defines a strict partial order between events. Two
  messages are concurrent if none happens before the other.
\end{definition}

\begin{definition}[\label{def:lwo}Last win order and staleness]
  When a \process $p$ broadcasts two messages $b_p(m) \rightarrow
  b_p(m')$, no \process $q$ can deliver $m$ if it has delivered $m'$:
  $\not\exists q \in V$ with $d_q(m') \rightarrow d_q(m)$, for $m$
  convey \emph{stale} control information.
\end{definition}

\input{input/figdel.tex}

\input{input/figproblem.tex}


Unfortunately, stale control information as stated in
Definition~\ref{def:lwo} may impair the propagation of both
\begin{inparaenum}[(i)]
\item notifications about actual sources, and
\item notifications about deleted partitions.
\end{inparaenum}
Figure~\ref{fig:problem} highlights such consistency issue with
dynamic partitions, even in contexts where \processes have FIFO links,
\ie where \processes receive the messages in the order of their
sending. Both \Process~$a$ and \Process~$d$ add then delete their
partition concurrently. \Process~$c$ receives, delivers, and forwards
$\alpha_d^3$ followed by $\delta_d$. In the meantime, \Process~$b$
receives, delivers, and forwards $\alpha_a^2$. In
Figure~\ref{fig:problemC}, both \Process~$c$ and \Process~$d$ deliver
the message about Partition $P_a$, for they did not have any known
partition at receipt time. On the contrary, \Process~$b$ delivers
$\alpha_d^1$, for it improves its distance to a known
source. \Process~$b$ then blocks \Process~$a$'s removal notification
$\delta_a$. It never reaches \Process~$c$ nor \Process~$d$. Also,
\Process~$c$ does not deliver $\alpha_d$ and $\delta_d$ since it
already delivered it. The system converges to an inconsistent state
where some \processes assume they belong to a partition while it does
not exist anymore. Naive propagation of $\alpha$ and $\delta$ messages
is insufficient to guarantee consistent partitioning when any \process
can add or delete its partition at any time. \Processes need to
exploit their local knowledge gathered over message receipts.


One could guarantee consistent partitioning by always propagating the
$\delta$ messages corresponding to the $\alpha$ messages it propagated
before. In Figure~\ref{fig:problem}, it means that as soon as
\Process~$b$ forwards $\alpha_a^2$, it assumes that its neighbors
\Process~$c$ and \Process~$d$ may need the notification of removal
$\delta_a$ if it exists. However, such solution also implies that
\processes generate traffic not only related to their current
partition, but also related to partitions they belong to in the
past. This would prove overly expensive in dynamic systems where
\processes join or leave the system, create or delete partitions, at
any time.  Instead, we propose to use the delivery order at each
\process to detect possible inconsistencies and solve them. Together,
\processes eventually remove all stale control information (transiting
messages and local states) of the system leaving room for propagation
of messages about up-to-date partitions.

\begin{corollary}[\label{theo:dcp}purge + BEF $\implies$
    \underline{D}ynamic \underline{c}onsistent
    \underline{p}artitioning (DCP)]
%
When each \process can \texttt{Add} and \texttt{Del}ete its partition
at any time, consistent partitioning requires
\begin{inparaenum}[(i)]
\item eventual purging of stale messages ($b_p(m) \rightarrow b_p(m')
  \implies \eventually (d_q(m) \implies d_q(m) \rightarrow
  d_q(m''))$);
\item and best eventual forwarding.
\end{inparaenum}
\end{corollary}

\begin{proof}
  \Processes deliver and forward the best sum of weights they
  received. Figure~\ref{fig:proofA} depicts the only case where a
  \process would block such epidemic propagation: the \process
  received and delivered a message from a partition that has since
  then been deleted but with a better sum of weights than newly
  received ones (in the example $\alpha_d^y <
  \alpha_a^{x'}$). Removing \emph{forever} all such stale messages
  about deleted partitions would allow \processes to propagate their
  best up-to-date partition again, eventually reaching consistent
  partitioning together, as stated by Theorem~\ref{theo:bef}.
\end{proof}

\input{input/figproof.tex}

%% Figure~\ref{fig:proof} depicts the issues with staleness and message
%% orderings. In Figure~\ref{fig:proofA}, the shortest path from any
%% source to \Process $c$ is $[a, b, c]$. However, \Process $b$ still holds
%% a stale $\alpha_d^{0.5}$ without knowing. When it receives
%% $\alpha_a^1$, it discards it, for it assumes that downstream \processes
%% are more interested in $\alpha_d^{0.5}$. To reach consistent
%% partitioning, \Process $b$ first needs to purge its current partition
%% to later accept that of its current actual shortest path:
%% $\alpha_a^1$.

%% \noindent Figure~\ref{fig:proofB} shows that removing stale control
%% information is even more complex. The removal must reach all \processes
%% of the previous shortest path going from $d$ to $b''$. Label I' shows
%% the most obvious issue where \Process $e$ changed partition for a
%% better but stale $\alpha_f$. Since it can remember its previous
%% deliveries, it could still forward $\delta_d$ for the sake of
%% consistency. However, this would lead to every \process forwarding
%% every $\delta$ they ever delivered. Such protocol's overhead would
%% depend on past partitioning instead of current one. Label III shows
%% the issue when the blocking partition is already known to be stale at
%% \Process $b'$. \Process $b''$ eventually receives $\alpha_f$ from
%% $e$. However, it cannot deliver it, for it would break last win order.
%% \Process $b'$ may be in an inconsistent state. Label IV shows the
%% corollary issue: \Process $b''$ delivered a message from a \process that
%% may be inconsistent without knowing it.

\begin{theorem}[three-phase purge] % Propagations $+$ detection $\implies$ purge]
  Purging stale control information from its source to every \process
  that may have delivered it last requires three
  phases:
  \begin{inparaenum}[(i)]
  \item propagation of delete notifications,
  \item detection of possible blocking conditions, and
  \item propagation of possibly deleted but blocked notifications.
  \end{inparaenum}
\end{theorem}

\begin{proof}
  Figure~\ref{fig:proofB} depicts all possible cases that need
  appropriate purging. A chain of \processes $[d, e, f, g]$ delivered
  and forwarded a message $\alpha_d$. Some \processes may have
  delivered messages about other partitions before or after
  $\alpha_d$.
  \begin{asparadesc}
  \item [\processes with last $\alpha_d$:] \Processes such as
    \Process~$d$ where the last and best partition is still $P_d$. By
    propagating from \process to \process the corresponding $\delta_d$
    message, these \processes purge $\alpha_d$.
  \item [\processes with $\alpha_d^y \rightarrow \alpha_h^z$ with last
    $\alpha_h^z$, for $\alpha_h^z < \alpha_d^y$:] \Processes such as
    \Process~$e$ do not directly suffer from the non-delivery of
    $\delta_d$. They need to purge their own $P_h$ if need
    be. Nonetheless, they deliver and forward $\alpha_h$ that at least
    one following \process will receive:
  \item [\processes with $\delta_h \rightarrow \alpha_d$:] \Processes
    such as \Process~$f$ do not belong to the preceding category, for
    they already delivered $\delta_h$. However, best eventual
    forwarding guarantees that they receive $\alpha_h$ that
    contradicts their history or state. In such case, the detecting
    \processes purge their own $\alpha_d$. The detecting \processes
    must also notify subsequent \processes that their current
    partition \emph{may be} deleted, for their parent may have blocked
    the corresponding $\delta$ messages. We note such notification
    $\delta_d?$ to emphasize the uncertainty of a $\delta$ message.
  \item [\processes with last $\alpha_d$ receiving $\delta_d?$ from
    their parent:] \Processes such as \Process~$g$ have $\alpha_d$ and
    preceding delivered messages have not importance. Such \processes
    must trust detecting \processes by delivering and forwarding
    $\delta_d?$. Messages are subject to all aforementioned blocking
    conditions, but are also solved by aforementioned mechanisms.
  \end{asparadesc}

  Propagation~I' terminates: a \process do not deliver a $\delta$
  message if it has already delivered it. Propagation~IV terminates: a
  \process do not deliver a looping message.
  % ; and since \processes forward the messages they deliver, upstream
  % \processes eventually receive and may deliver messages detected as
  % issue. In Figure~\ref{fig:proofB}, \Process~$e$ eventually
  % receives $\delta_f$ from \Process~$b'$.
  These three phases propagation-detection-propagation ensure that
  \processes eventually purge stale $\alpha$ messages from the system,
  leaving room for Propagation~I.
  %
  %  \TODO{cannot loop add from $e$ , undo from $b'$ because $b'$ sent
  % delete to $e$, it will solve the inconsistency.}
  %
  %% In the meantime, \processes that still belong to a partition can
  %% propagate their respective last $\alpha$ message, and reach
  %% consistent partitioning.
\end{proof}



\vfill

\TODO{spacing.}

\newpage

\subsection{Implementation and complexity}

Algorithm~\ref{algo:ascast} provides the few instructions of \NAME
that implement three-phase purge and best eventual forwarding to
enable dynamic consistent partitioning.

%% Similarly to Algorithm~\ref{algo:add}, local operations \texttt{Add}
%% and \texttt{Del} generate messages that the initiating \process treats
%% identically to receipts of messages from remote \processes.

%% \begin{inparaenum}[(i)]
%% \item \label{algo:most} most of the time, it propagates $\alpha$'s and
%%   $\delta$'s when the partitions allow it;
%% \item \label{algo:sometimes} sometimes, it detects possible
%%   inconsistent partitioning and
%% \item \label{algo:solves}solves it using propagation trees (as opposed
%%   to propagation graphs);
%% \item \label{algo:competition} when required, it triggers competitions
%%   among neighboring partitions.
%% \end{inparaenum}
%% While (\ref{algo:most})-(\ref{algo:solves}) tackle the eventual purging of
%% stale notifications, (\ref{algo:competition}) tackle the eventual
%% delivery of the best up-to-date partitions.

\begin{algorithm}
  \input{input/algoascast.tex}
  \caption{\label{algo:ascast}\NAME at \Process~$p$ in static networks.}
\end{algorithm}

To implement last win order as stated in Definition~\ref{def:lwo},
each \process maintains a vector of versions that associates the
respective known local counter of each known source, or has-been
source.  Each \process delivers and forwards
\begin{inparaenum}[(i)]
\item only newest messages (see Line~\ref{line:ascast_version}) that
\item improve their best known partition (see
  Line~\ref{line:ascast_better}).
\end{inparaenum}
This vector enables both Propagation~I and Propagation~I'.  The size
of such vector increases linearly with the number of sources that the
\process ever known which is the number of \processes in the system
$\mathcal{O}(V)$ in the worst case. Unfortunately, \processes cannot
remove any entry from this vector without running a costly distributed
consensus protocol to ensure that no \process can mistake an old but
removed entry for a new one. However, following the principles of
scoped broadcast, we expect that every \process knows only a small
subset of sources.

This vector of versions constitutes a summary of deliveries. \NAME
uses it to detect possible inconsistent partitioning as depicted in
Figure~\ref{fig:proofB}. A detecting \process then propagates a
$\delta$ message conveying the unique identifier $s$ and version $c$
of the partition along with a path $\pi$ comprising the identity of
\processes that forwarded the corresponding $\alpha$ message. Such
$\delta$ message implements the uncertain $\delta?$ message of
Figure~\ref{fig:proofB}. It fulfils the double role of removing stale
control information without modifying the local counter of the source
(since the partition may still exist), and removing loops to ensure
termination (since \processes do not modify their local state to
reflect such delivery, messages piggyback monotonically such
increasing control information). Thus, $\delta$ messages used in
Propagation~I' are more efficient both in propagation time and
traffic, for they propagate using every communication link available
in the partition, and they do not need to carry their path.
Fortunately, Propagation~I' happens more often than Propagation~IV.
Yet, \processes that belong to a partition need to save the path taken
by the corresponding $\alpha$ messages to enable Propagation~IV.  In
the worst case, the path includes every \processes in the system but
converges in average to the average diameter of partitions. Thus,
message paths and \NAME have an interesting synergy: Paths tend to be
smaller as the number of sources in the system increases. Bloom
filters could introduce another trade-off by further decreasing the
size of path data structures at the cost of premature stopped
propagation that could impair consistent
partitioning~\cite{whitaker2002forwarding}.

As stated in Theorem~\ref{theo:dcp}, dynamic consistent partitioning
not only requires eventual purging of stale control information, but
also the retrieval of the best up-to-date partitions. In that regard,
both kinds of $\delta$ messages have dual use: since they already
reach the borders of their partition when they remove stale control
information, they also trigger a competition when reaching such
bordering \processes (see Line~\ref{line:ascast_compete}). This simply
consists in sending the current partition through the communication
link from which the \process received the $\delta$. Upon receipt of
this answer, \processes act normally by propagating their changes when
they improve. \TODO{Exemple with figure for detection?}

\TODO{Rework this.}\NAME guarantees dynamic consistent partitioning by
making extensive use of \process to \process communications. In terms
of number of messages, in the average-case, a \process $i$ chosen
uniformly at random among all \processes creates a logical
partition. Its messages $\alpha_i$ propagate through the network until
reaching \processes that belong to another partition closer to
them. This splits partitions in half in average. Overall, the $a^{th}$
new partition comprises \smash{$\mathcal{O}(\frac{|V|}{2^{\lfloor
      \log_2 a \rfloor}})$} \processes. This decreases every new
partition until reaching $0$ \processes per new partition: even the
chosen \process already belongs to its optimal partition. The average
number of messages per \process is
\smash{$\mathcal{O}(\frac{\overline{|O|}}{2^{\lfloor \log_2 a
      \rfloor}})$}. \TODO{Multiple receipt and multiple delivery imply
  more messages (receipt bounded by $|O|$ as well).} Deleting the
$a^{th}$ partition generates the exact same number of messages than
the $a^{th}$ partition creation. \TODO{But what about echos?} In the
worst-case, every new partition includes all but one \process
belonging to the previous partition. The total number of messages
after the $a^{th}$ new partition is $\mathcal{O}(\overline{|O|}\cdot
a^2)$. As for the average-case, the number of messages for the
partition deletion is identical to the number of messages of the
corresponding partition creation.

\begin{algorithm}
  \input{input/algoedges.tex}
  \caption{\label{algo:edges}\NAME at \Process~$p$ in dynamic networks.}
\end{algorithm}

Algorithm~\ref{algo:edges} extends the instructions of \NAME that
enable dynamic consistent partitioning in dynamic networks where
\processes can join or leave the system without notification. We
consider that removing a \process is equivalent to removing all its
edges, and adding a \process is equivalent to add as many edges as
necessary.

Adding an edge between two \processes may only improve the shortest
path of one of these \processes. Therefore, it triggers a competition
between the two \processes only. Removing an edge between two
\processes may invalidate the shortest path of one of involved
\processes plus downstream \processes. To implement this behavior,
\NAME uses the implementation of $\delta?$ messages. This prove costly
but enables \NAME even in dynamic networks subject to physical
partitioning.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 
