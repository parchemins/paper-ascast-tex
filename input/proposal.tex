
%% to better adapt positioning (I expect 2.5 pages
%% intro+related work)

\vfill \TODO{spacing for intro and related work: 2.5 pages}
\newpage

\section{Scoped broadcast}
\label{sec:scoped}

Scoped broadcast is a communication primitive that propagates a
message around its broadcaster within an application-dependant scope.
For instance, a process from Paris could scoped broadcast messages to
all processes in Paris; and processes from other cities would never
deliver such messages. Scoped broadcast allows its implementations to
reduce generated traffic compared to uniform reliable
broadcast~\cite{hadzilacos1994modular, raynal2013distributed}, for
message propagation can stop as soon as messages reach the borders of
the scope.  This section introduces model and notations to provide a
general definition of scoped broadcast in distributed
systems. \TODO{to help understand adaptive scoped broadcast :).}
\TODO{Better transition to the rest of this section.}

Distributed systems such as Edge infrastructures comprise
heterogeneous machines interconnected by communication links. We do
not consider byzantine processes.

\begin{definition}[Fog infrastructure]
  A fog infrastructure is a connected \underline{g}raph $G(V, E)$ of
  \underline{v}ertices $V$ and bidirectional \underline{e}dges $E
  \subseteq V \times V$.  A \underline{p}ath $\pi_{ij}$ from Process
  $i$ to Process $j$ is a sequence of vertices $[i, k_1, k_2, \ldots
    k_n, j]$ with $\forall m: 0\leq m \leq n, \langle \pi_{ij}[m],
  \pi_{ij}[m+1] \rangle \in E$.
\end{definition}

Processes can send messages to their neighboring processes. Processes
can reliably broadcast messages by forwarding delivered
messages~\cite{birman1999bimodal, hadzilacos1994modular,
  nedelec2018causal, raynal2013distributed}. However, uniform reliable
broadcast states that \emph{all} correct processes must deliver
broadcast messages. This proves costly in large scale systems
comprising thousands of processes. Instead, we define \emph{scoped
broadcast} where messages reach only interested processes,
significantly reducing generated traffic.

\begin{definition}[\label{def:scoped}\underline{S}coped broad\underline{cast} (\NAMEB)]
  When Process $x$ scoped \underline{b}roadcasts $b_x(m)$ a
  \underline{m}essage $m$, every correct Process $y$ within a scope
  \underline{r}eceives $r_y(m)$ and \underline{d}elivers it
  $d_y(m)$. The scope depends on the \underline{s}tate $\sigma$ of
  each process, the \underline{m}etadata $\mu$ piggybacked by each
  message, and a \underline{p}redicate $\phi$ verified from process to
  process: $b_x(m) \wedge r_y(m) \implies \exists \pi_{xy}: \forall z
  \in \pi_{xy}, \phi(\mu_z, \sigma_z)$.
\end{definition}

This definition highlights the transitive relevance of messages and
encompasses more specific definitions of related
work~\cite{hsiao2005scoped, lue2006scoped, wang2015prodiluvian}. For
instance, a process from Paris could scoped broadcast messages to all
processes in Paris. This requires processes to know the city they are
in. In another instance, a process from Paris could scoped broadcast
messages to all processes in Paris plus neighboring cities. This
requires processes to overload forwarded messages the first time they
reach another city. The predicate checks if messages already reached
two distinct cities before delivery. Similarly to uniform reliable
broadcast, scoped broadcast implementations expose different
trade-offs on space, time, and communication.

Next Section introduces an adaptive scoped broadcast where the state
of each process is dynamic and changes depending on all distinct
scopes that exist in the system.



\section{Adaptive scoped broadcast}
\label{sec:adaptive}

In this section, we propose to dynamically adapt the scopes of \NAMEB
using \NAMEB itself for the sake of efficiency. At any time, a process
decides whether or not it becomes the center (or \emph{source}) of a
scope (or \emph{partition}). Processes must eventually rally the
partition corresponding to their closest source despite concurrent
operations, and dynamic changes in the network.

\noindent In that regard, we introduce \NAME (stands for
\underline{A}daptive \underline{S}coped broad\underline{cast}), a
wait-free reactive protocol for dynamic logical partitioning that
enables adaptive scoped broadcast in dynamic distributed
systems. \NAME's operation guarantees that processes eventually reach
consistent partitioning by efficiently propagating their state
changes. \NAME's overhead actually depends on its operations and
current partitions in the system. In other terms,
\begin{inparaenum}[(i)]
\item when the system becomes quiescent, processes eventually converge to
their respective partition and do not require further communication
afterward; and
\item processes do not pay the price of past partitioning.
\end{inparaenum}

\noindent This section starts by introducing consistent partitioning
and its implementation when a process can only become a new source to
create a new partition, i.e., it cannot revoke its self-appointed
status of source. Then, it extends to include the latter behavior: it
introduces the properties that guarantee consistent partitioning in
such context, and it details our implementation called \NAME. Finally,
it provides the complexity analysis of \NAME divided in time, space,
and communication.


\subsection{Consistent partitioning}

At any time, a process can decide to become a source, hence creating a
new partition in the system by executing an \texttt{Add}
operation. This partition includes at least its source plus
neighboring processes that estimate they are closer to this source
than any other source. Such distance (or \emph{weight}) is
application-dependant. For instance, weights could be latency in live
streaming applications, or differences in profiles for recommendation
systems.

\begin{definition}[\label{def:partitioning}Partitioning]
  Let $S \subseteq V$ be the set of \underline{s}ources, and $P_s$ be
  the \underline{p}artition including at least Process $s$, each
  process belongs to at most one partition $\forall p,q \in V, \forall
  s_1,s_2 \in S: p \in P_{s_1} \wedge q \in P_{s_2} \implies p \neq q
  \vee s_1 = s_2$, and there exists at least one path $\pi_{ps}$ of
  processes that belong to this partition $\forall q \in \pi_{ps}: q
  \in P_s$.
\end{definition}

Definition~\ref{def:scoped} and Definition~\ref{def:partitioning}
share the transitive relevance of process states. However, we further
constrain the partitioning in order to guarantee the existence of
exactly one consistent partitioning that processes eventually converge
to.

\begin{definition}[\underline{C}onsistent \underline{p}artitioning (CP)]
  Let $W_{xy} = W_{yx}$ be the symmetric \underline{w}eight between
  $x$ and $y$, $\Pi_{xz}$ be the shortest \underline{p}ath from $x$ to
  $z$ the weight of which $|\Pi_{xz}|$ is lower than any other path
  weight, the only consistent partitioning $\mathcal{P}$ is a set of
  partitions $P_{s\in S}$ such that each process belongs to a logical
  partition comprising its closest source: $\forall p \in P_{s_1},
  \nexists P_{s_2}$ such that $|\Pi_{s_1p}| > |\Pi_{s_2p}|$.
\end{definition}

Unfortunately, processes do not share a common global knowledge of the
network state. For processes to reach consistent partitioning
together, a process becoming a source must notify every process that
is closer to it than any other source. Epidemic
dissemination~\cite{birman1999bimodal} constitutes an efficient
communication pattern to perform such operation. \TODO{More about
  \emph{forwarding}.}

\begin{theorem}[\underline{B}est \underline{e}ventual \underline{f}orwarding $(BEF) \iff CP$]
  Assuming reliable communication links where a correct process $q$
  eventually receives the message $m$ \underline{s}ent to it by a
  process $p$ ($s_{pq}(m) \implies r_{q}(m)$); a static system ($G^t =
  G^{t+1}$); assuming local knowledge only, processes eventually reach
  consistent partitioning if and only if each process eventually
  forwards its best known partition.
\end{theorem}

\TODO{If forwarding defined as send to all neighbors, then it also
  works in dynamic systems where processes join and edges are added,
  for when a new neighbor appears, the processes must forward their
  best.}

\begin{proof}
  \begin{asparadesc}
  \item [$BEF \implies CP$:] When a process $s_1$ becomes a source, it
    belongs to its own partition, for there exists no better partition
    than its own: $\forall p \in V: |\Pi_{s_1 s_1}| < |\Pi_{s_1
      p}|$. It broadcasts such message to its neighbors. Since
    communication channels are reliable, neighboring processes
    eventually receive such notification. In particular, whatever the
    order of received messages, every process $p_1$ -- such that there
    exists no better partition $s_2$ than the received one -- delivers
    and forwards it: $\forall s_2 \in S: |\Pi_{p s_1}| < |\Pi_{p
      s_2}|$. The notification emanating from $s_1$ transitively
    reaches all such processes through shortest paths. Since there
    exists only one best value per process that can never be
    retracted, processes eventually reach consistent
    partitioning. \TODO{Rework this with $s_{pq}$ and $r_{pq}$.}
  \item [$CP \implies BEF$:] \TODO{By contradiction, if there is a
    process $p$ that does not forward, then another process $q$ may
    never gets its best because it needs the weight $W_{pq}$.}
  \end{asparadesc}
\end{proof}

\begin{algorithm}
  \input{input/algoadd.tex}
  \caption{\label{algo:add}Adding a partition by Process $p$.}
\end{algorithm}

\input{input/figadd.tex} 

Algorithm~\ref{algo:add} shows the instructions that implement such
consistent partitioning when
\begin{inparaenum}[(i)]
\item weights are scalar values,
\item processes only add new partitions to the system,
\item and processes never join nor crash nor leave the system.
\end{inparaenum}
Figure~\ref{fig:add} illustrates its behavior on a system comprising 4
processes $a$, $b$, $c$, and $d$. Process~$a$ and Process~$d$ become
the sources of their partition. They \NAMEB a notification
\underline{a}dd message: $\alpha_a^0$ and $\alpha_d^0$. They
initialize their own state with the lowest possible bound $0$ (see
Line~\ref{line:lowestbound}), and send a message to each of their
neighbors by accumulating the corresponding edge weight (see
Line~\ref{line:accumulator}). In Figure~\ref{fig:addC}, Process~$b$
receives $\alpha_{d}^{1}$. Since it improves its own partition
distance, it keeps it and forwards it to its neighbors. In
Figure~\ref{fig:addD}, Process~$b$ discards $\alpha_{a}^{2}$, for it
does not improve its partition distance. Processes $c$ and $d$ will
never hear of Source~$a$. Ultimately, processes discard last
transiting messages. The system reached consistent partitioning.

%% However, since processes operate using local knowledge only, generated
%% traffic may be sub-optimal, for processes may receive, deliver, and
%% forward messages emanating from a same source simply because it did
%% not come from the shortest path yet.

%% Transitive relationships ensure that each
%% process gets its closest source while accumulation of weights ensures
%% that messages propagation terminates.

%% Interestingly, while the receipt order of messages does not impact on
%% local control information ($r_b(\alpha_a^{1.5}) \cdot
%% r_b(\alpha_a^{1}) \Leftrightarrow r_b(\alpha_a^{1}) \cdot
%% r_b(\alpha_a^{1.5}) \implies \TODO{d_b(\alpha_a^{1})}$), it impacts on
%% control information broadcast in the network ($r_b(\alpha_a^{1.5})
%% \cdot r_b(\alpha_a^{1}) \implies b_b(\alpha_{b}^{1.5}) \cdot
%% b_b(\alpha_{b}^{1})$ while $r_b(\alpha_a^{1}) \cdot
%% r_b(\alpha_a^{1.5}) \implies b_b(\alpha_a^{1})$).
 
While only adding logical partitions to the distributed system is
straightforward, removing them introduces additional complexity:
messages of old partitions may stop the notification of best sources,
for former weights are better than the newly received one. 


\subsection{Dynamic consistent partitioning}

At any time, a process can become a source, hence adding a new
partition to the system. This partition eventually includes all
processes that are closer from this new source than any other
else. Processes naturally converge towards their respective best
partition by only piggybacking a monotonically increasing distance in
forwarded messages.

Then, at any time, a source can revoke its self-appointed status of
source, hence deleting its partition from the system, by executing a
\texttt{Del} operation. All processes that belong to this partition
must eventually choose another partition to belong to. In
Figure~\ref{fig:del}, two partitions exist initially: $P_a$ and $P_c$
that respectively include $\{a\}$, and $\{b, c, d\}$. In
Figure~\ref{fig:delA}, Process $a$ deletes its partition. It notifies
all neighboring processes -- here only Process $b$ -- that may belong
to its partition using \NAMEB. Upon receipt, Process $b$ discards the
message $\delta_a$, for the latter does not target the former's
partition. Yet, Process $b$ sends its own partition that may be the
best for Process $a$. Eventually, every processes belong to the same
partition $P_c$.

\input{input/figdel.tex}

Unfortunately, stale control information about deleted partitions may
impair the propagation of both
\begin{inparaenum}[(i)]
\item notifications about actual sources, and
\item notifications about other deleted partitions.
\end{inparaenum}
Figure~\ref{fig:problem} highlights such consistency issue with
dynamic partitions, even in contexts where processes have FIFO links,
\ie where processes receive the messages in the order of their
sending. Both Process~$a$ and Process~$d$ add then delete their
partition concurrently. Process~$c$ receives, delivers, and forwards
$\alpha_d^3$ followed by $\delta_d$. In the meantime, Process~$b$
receives, delivers, and forwards $\alpha_a^2$. In
Figure~\ref{fig:problemC}, both Process~$c$ and Process~$d$ deliver
the message about Partition $P_a$, for they did not have any known
partition at receipt time. On the contrary, Process~$b$ delivers
$\alpha_d^1$, for it improves its distance to a known
source. Process~$b$ then blocks Process~$a$'s removal notification
$\delta_a$. It never reaches Process~$c$ nor Process~$d$. Also,
Process~$c$ and Process~$d$ do not deliver any $\alpha_d$ or
$\delta_d$ since they already delivered it. The system converges to an
unconsistent state where some processes assume they belong to a
partition while it does not exist anymore. Naive propagation of
$\alpha$ and $\delta$ messages is insufficient to guarantee consistent
partitioning when each process can add or delete its partition at any
time.

To identify and solve the issue with dynamic consistent partitioning
where protocols enable processes to \texttt{Add} and \texttt{Del} at
any time, we need to further constrain the delivery order at each
process.

\input{input/figproblem.tex}

\begin{definition}[Happens-before $\rightarrow$~\cite{lamport1978time}]
  The transitive, irreflexive, and antisymmetric happens-before
  relationship defines a strict partial order between events. Two
  messages are concurrent if none happens before the other.
\end{definition}

We define a last win order to forbid the delivery of stale information
emanating from a same process. In other terms, the \emph{best}
notification message is always the most recent known of each process.
 
\begin{definition}[Last win order]
  When a process $p$ broadcasts two messages $b_p(m) \rightarrow
  b_p(m')$, no process $q$ can deliver $m$ if it has delivered $m'$:
  $\not\exists q \in V$ with $d_q(m') \rightarrow d_q(m)$.
\end{definition}

\input{input/figproof.tex}

Figure~\ref{fig:proof} depicts the issues with staleness and message
orderings. In Figure~\ref{fig:proofA}, the shortest path from any
source to Process $c$ is $[a, b, c]$. However, Process $b$ still holds
a stale $\alpha_d^{0.5}$ without knowing. When it receives
$\alpha_a^1$, it discards it, for it assumes that downstream processes
are more interested in $\alpha_d^{0.5}$. To reach consistent
partitioning, Process $b$ first needs to purge its current partition
to later accept that of its current actual shortest path:
$\alpha_a^1$.

\noindent Figure~\ref{fig:proofB} shows that removing stale control
information is even more complex. The removal must reach all processes
of the previous shortest path going from $d$ to $b''$. Label I' shows
the most obvious issue where Process $e$ changed partition for a
better but stale $\alpha_f$. Since it can remember its previous
deliveries, it could still forward $\delta_d$ for the sake of
consistency. However, this would lead to every process forwarding
every $\delta$ they ever delivered. Such protocol's overhead would
depend on past partitioning instead of current one. Label III shows
the issue when the blocking partition is already known to be stale at
Process $b'$. Process $b''$ eventually receives $\alpha_f$ from
$e$. However, it cannot deliver it, for it would break last win order.
Process $b'$ may be in an inconsistent state. Label IV shows the
corollary issue: Process $b''$ delivered a message from a process that
may be inconsistent without knowing it.

\begin{corollary}[\label{theo:dcp}Dynamic consistent partitioning]
  Eventual consistent partitioning in presence of dynamic sources
  requires
  \begin{inparaenum}[(i)]
  \item eventual purging of stale notifications, and
  \item eventual delivery of bests partitions.
  \end{inparaenum}
\end{corollary}

\begin{proof}
  \TODO{Quite obvious.}
\end{proof}

\begin{theorem}[\TODO{meow}]
  \TODO{Puring stale notifications requires to handle I...IV.}
\end{theorem}

\begin{proof}
  \TODO{Meow meow meow.}
  \TODO{Label I: default propagation to what seems the border of the partition but isn't.}
  \TODO{Label II: gates eventually break because of I', III, IV.}
  \TODO{Label I': as Label I.}
  \TODO{Label III: detecting possible inconsistent partitioning.}
  \TODO{Label IV: propagation of this issue to all and only concerned parties.}
\end{proof}



\subsection{Implementation}

\begin{algorithm}
  \input{input/algoadddelundo.tex}
  \caption{\label{algo:adddelundo}Dynamic partitioning by Process $p$.}
\end{algorithm}

We extend Algorithm~\ref{algo:add} to enable each source to revoke its
self-appointed status of source and become a simple process again.
Algorithm~\ref{algo:adddelundo} provides the instructions that
implement dynamic consistent partitioning:
\begin{inparaenum}[(i)]
\item \label{algo:most} most of the time, it propagates $\alpha$'s and
  $\delta$'s when the partitions allow it;
\item \label{algo:sometimes} sometimes, it detects possible
  inconsistent partitioning and
\item \label{algo:solves}solves it using propagation trees (as opposed
  to propagation graphs);
\item \label{algo:competition} when required, it triggers competitions
  among neighboring partitions.
\end{inparaenum}
While (\ref{algo:most})-(\ref{algo:solves}) tackle the eventual purging of
stale notifications, (\ref{algo:competition}) tackle the eventual
delivery of the best up-to-date partitions.

To constrain the order of delivery at each process, each process
implements last win order by maintaining a vector of versions that
associates to each known source, or has-been source, with its known
local counter. It enables processes to deliver only up-to-date
$\alpha$'s that may improve their best known partition (see
Line~\ref{line:detectA}), but also to detect possible inconsistent
partitioning (see Line~\ref{line:detectB}) as labeled by III in
Figure~\ref{fig:proofB}. From a global perspective, it ensures
monotonic increase of knowledge despite the non-monotonic number of
sources. In turns, it contributes to convergence and termination, for
corresponding $\alpha$ and $\delta$ cannot follow each other in an
infinite loop.

Implementing the propagation of deletes is no different from the
propagation of adds as hinted in Figure~\ref{fig:proof} with Labels I
and I'. Messages and comparisons include the local counter of the
source. When a process receives a better up-to-date partition, it
delivers and forwards it. When a process receives a delete
notification about its partition, it delivers and forwards it. It is
worth noting that such $\delta$'s apply to the whole partition. They
travel in the propagation graph of their partition using all
communication links, allowing processes to change partition as soon as
possible.

Vectors of versions also allow each process to detect possible last
win order violations. Upon possible inconsistency, the detecting
process must remove all $\alpha$ messages originating from it, as
labeled by IV in Figure~\ref{fig:proofB}. Contrarily to delete
operations, this applies to a sub-partition of the current partition,
and cannot rely on local counters to
operate. Algorithm~\ref{algo:adddelundo} implements this behavior by
piggybacking paths in messages. Paths guarantee that such $\delta$
messages apply to all and only $\alpha$'s that followed an identical
path. $\delta$'s propagate in the propagation tree rooted at the
detecting process. Local states of processes remain unchanged, but
paths in messages monotonically grow. In turns, $\alpha$ and $\delta$
cannot follow each other in an infinite loop, for a process knows when
it already received a forwarded a message (see
Lines~\ref{line:notloopingA} and \ref{line:notloopingB}).

\noindent It is worth noting that scoped broadcast and the
piggybacking of paths synergies well: Paths tend to be smaller as the
number of sources in the system increases.

Finally, as stated in Theorem~\ref{theo:dcp}, dynamic consistent
partitioning not only requires the removal of all stale partitions,
but also the retrieval of the best up-to-date partitions. In that
regard, $\delta$'s have dual use: since they already reach borders of
partitions when they remove stale control information, they also
trigger a competition when reaching such bordering processes (see
Line~\ref{line:compete}). This simply consists in sending the current
partition through the communication link from which the process
received the $\delta$. Upon receipt of this answer, processes act
normally by propagating their changes when they improve. These adds
fill the gaps left open by deletes.

\begin{algorithm}
  \input{input/algoedges.tex}
  \caption{\label{algo:edges}Dynamic partitioning by Process $p$ in dynamic networks.}
\end{algorithm}

Algorithm~\ref{algo:edges} provides the instructions of \NAME that
enable dynamic consistent partitioning in dynamic networks where
processes can join or leave the system without
notification. \TODO{More.}

%% Adding new communication links to the network may create shortcuts
%% between processes. Both processes must send their current best
%% partition to each other. Upon receipt, they act normally: if a process
%% finds out that the received partition is closer than its current one,
%% it delivers it which in turns also triggers another competition
%% amongst neighbors due to forwarding.

%% \noindent Joining the network is equivalent to add as many
%% communication links as necessary between the joining process and its
%% new neighbors.

%% When removing a communication link between two processes does not
%% break any active path, because neither distances of processes depend
%% on the other, then nothing needs to be done. \NAME has no overhead.
%% Unfortunately, when a process' distance depends on the other process,
%% the protocol becomes much more complex. Indeed, this requires to
%% \TODO{undo} all add messages originated from this process. A message
%% must convey the fact that
%% \begin{inparaenum}[(i)]
%% \item an edge at a particular process has been removed, and
%% \item the distance that has been delivered by a process comes from
%%   this particular process.
%% \end{inparaenum}



%% \subsection{Inter-autonomous system partitioning}

%% \begin{figure}
%%   \centering\input{input/figAS.tex}
%%   \caption{\label{fig:AS}Inter autonomous systems
%%     partitioning. \TODO{More about relative ordering of links at each
%%       process}}
%% \end{figure}

%% In this section, we aim at solving Problem Statement~\ref{prob:inter}
%% by proposing \NAMEB, an extension of \NAME, that enables dynamic
%% logical partitioning in inter-autonomous systems. It leverages
%% hierarchical properties of these networks to further improve on scoped
%% flooding.

%% Autonomous systems are geo-distributed networks. Heterogeneous
%% communication links. Hierarchy of communication links. Per-object
%% broadcast.

%% We leave the link handling to membership protocols~\REF. The ordering
%% of links can be done based on latency. For instance, from 0 to 100 ms,
%% rank 0, from 100 to 1s, rank 2 \ldots

%% \begin{algorithm}
%%   \input{input/algoaaaa.tex}
%%   \caption{\label{algo:aaaa}\NAMEB running at Process $p$ for dynamic
%%     partitioning in inter-autonomous systems.}
%% \end{algorithm}

%% Figure~\ref{fig:AS} shows an example of inter-AS logical partitioning
%% where each process ranks its communication links, and each process
%% forwards messages to its neighbors starting from the rank of the link
%% that triggered the receipt, to its highest rank neighbors. In this
%% example, Process $e$ adds a partition. It forwards the generated
%% message to all its neighbors as if it received it from its lowest rank
%% links. When Process $a$ receives $\alpha_e^1$ from its rank-$0$ link,
%% it forwards it to rank-$0$ links and rank-$1$ links.  Process $b$
%% stops the propagation, for all its links have lower ranks than the one
%% from which it received the message $\alpha_e^2$. Ultimately, the
%% partition includes Process $e$, Process $a$, and Process $b$. Process
%% $c$ and Process $d$ remain unaware of the new \TODO{object}.

%% To ease the reasoning about inter-AS dynamic partitioning in dynamic
%% networks, \NAME runs an independent broadcast protocol for each
%% object, and for each rank of links.  \TODO{Not so easy\ldots}
%% \TODO{Write and describe algo.} \TODO{Overhead of separating things,
%%   slightly more memory, but traffic wise? slower ?}  \TODO{In the
%%   example, not only Process $b$ keeps $e_1$ as its best partition, but
%%   it keeps it for links of rank 0, and links of rank 1.}  When there
%% are no ranking in links, processes solely rely on
%% Algorithm~\ref{algo:adddelundo} for optimal partitioning.





\subsection{Complexity}

We focus on average-case and worst-case complexity. We divide our
analysis into space, time, and communication complexity.

\paragraph{Space.}
Each process needs to maintain a vector of versions the size of which
depends on the number of sources. In the worst case, the size of this
vector increases linearly with the number of distinct sources that
ever existed in the system. For the sake of last win order, processes
can never purge this vector from any entry. \TODO{Even when the
  process leaves the system.} Fortunately, scoped broadcast limits
message propagation to interested processes. A process at a side of
the system may never hear of a source at the other side of the
system. In turns, it may never have to maintain an entry for this
source. \TODO{This is application-dependent?}

\noindent Each process must also maintain the partition it belongs to,
along with its control information. Among other, it must maintain a
path the size of which depends on the membership size of the
partition. In the worst case, this path includes the identity of every
process of the system that currently belong to the system. Eventually,
the size of this path increases linearly with the diameter of the
current partition. \TODO{assuming weight equals 1 \ldots}

\paragraph{\TODO{Time.}}
In terms of processing time, whatever the operation, it increases
linearly with the number of neighbors of the process. It scales well,
for the number of neighbors is commonly order of magnitude lower than
the number of processes in the system. For instance, random networks
only require $O(\log(|V|))$ neighbors to work properly \REF; and
processes in autonomous systems have in average \TODO{X} neighbors
\REF. Lines~\ref{line:notloopingA} and \ref{line:notloopingB} can
prove costly depending on the data structure encoding paths. Using
Bloom filters however, this is \TODO{efficient}: it depends on the
number of buckets, and the number of hash functions.

\noindent In terms of convergence time, every $\alpha$ needs to travel
at least its shortest path before termination. Every $\delta$ needs to
reach the border of its partition, triggering an $\alpha$ that fills
the gaps left open by the removal. \TODO{More.}

\paragraph{\TODO{Communication.}}
In terms of number of messages required to reach optimal
partitioning. In the average-case, a process $i$ chosen uniformly at
random among all processes creates a logical partition. Its messages
$\alpha_i$ propagate through the network until reaching processes that
belong to another partition closer to them. This splits partitions in
half in average. Overall, the $a^{th}$ new partition comprises
\smash{$\mathcal{O}(\frac{|V|}{2^{\lfloor \log_2 a \rfloor}})$}
processes. This decreases every new partition until reaching $0$
processes per new partition: even the chosen process already belongs
to its optimal partition. The average number of messages per process
is \smash{$\mathcal{O}(\frac{\overline{|O|}}{2^{\lfloor \log_2 a
      \rfloor}})$}. \TODO{Multiple receipt and multiple delivery imply
  more messages (receipt bounded by $|O|$ as well).} Deleting the
$a^{th}$ partition generates the exact same number of messages than
the $a^{th}$ partition creation. \TODO{But what about echos?} In the
worst-case, every new partition includes all but one process belonging
to the previous partition. The total number of messages after the
$a^{th}$ new partition is $\mathcal{O}(\overline{|O|}\cdot a^2)$. As
for the average-case, the number of messages for the partition
deletion is identical to the number of messages of the corresponding
partition creation.

\noindent \TODO{In terms of size of messages.}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 
