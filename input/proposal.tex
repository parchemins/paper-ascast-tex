
\section{Adaptive scoped broadcast}
\label{sec:adaptive}

To assign and maintain \processes to their best partition according to
replica creations and removals, as well as dynamic infrastructure
changes, we designed and implemented \NAME.  \NAME stands for
\underline{A}daptive \underline{S}coped broad\underline{cast}. It
relies on a primitive that allows a \process to broadcast a message
within a limited scope. We first use this primitive to guarantee
consistent partitioning when a \process can only create a new replica
within the system. We highlight the issue when a process can also
destroy a replica, and provide a second algorithm that handles replica
removals as well as dynamic changes of the infrastructure.  This
section describes the communication primitive called scoped broadcast,
then discusses the properties that guarantee consistent partitioning
in our context, and finally details our implementation \NAME and its
complexity.


\subsection{Scoped broadcast}
\label{subsec:scoped}

%For instance, a \process from Paris could scoped broadcast messages to
%all \processes in Paris; and \processes from other cities would never
%deliver such messages.
%Scoped broadcast only targets a connected \emph{subset} of \processes
%in the whole distributed system.
%

% \TODO{dynamic graph: superscript with $t$?}
In this paper, we consider Edge infrastructures with heterogeneous
\nodes interconnected by communication links. \Processes involved in
the management of content may crash but are not byzantine.  Finally,
\processes can reliably communicate through asynchronous message
passing to other known \processes called neighbors.  % A more formal
%definition of our system is given below:

\begin{definition}[Edge infrastructure]
  An Edge infrastructure is a connected \underline{g}raph $G(V, E)$ of
  \underline{v}ertices $V$ and bidirectional \underline{e}dges $E
  \subseteq V \times V$.  A \underline{p}ath $\pi_{ij}$ from
  \Process~$i$ to \Process~$j$ is a sequence of vertices $[i, k_1,
    k_2, \ldots k_n, j]$ with $\forall m: 0\leq m \leq n, \langle
  \pi_{ij}[m], \pi_{ij}[m+1] \rangle \in E$.
\end{definition}


%Using epidemic
%propagation (or gossiping), \processes can reliably broadcast messages
%by forwarding delivered messages from neighbors to
%neighbors~\cite{birman1999bimodal, hadzilacos1994modular,
%  nedelec2018causal, raynal2013distributed}.
%
%However, uniform reliable
%broadcast states that \emph{all} correct \processes must deliver all
%broadcast messages. This proves costly in large scale systems
%comprising thousands of \processes. Instead, we define \emph{scoped
%broadcast} where messages reach only an application-dependant subset
%of connected \processes, thus significantly reducing the generated
%traffic of broadcast.

We define scoped broadcast as a communication primitive that
propagates a message around its broadcaster within an
application-dependant scope:


\begin{definition}[\label{def:scoped}\underline{S}coped broad\underline{cast} (\NAMEB)]
  When \Process~$x$ scoped \underline{b}roadcasts $b_x(m)$ a
  \underline{m}essage $m$, every correct \process $y$ within a scope
  \underline{r}eceives $r_y(m)$ and \underline{d}elivers it
  $d_y(m)$. The scope depends on the \underline{s}tate $\sigma$ of
  each \process, the \underline{m}etadata $\mu$ piggybacked by each
  message, and a \underline{p}redicate $\phi$ verified from \process to
  \process: {\small{$b_x(m) \wedge r_y(m) \implies \exists \pi_{xy}: \forall z
  \in \pi_{xy}, \phi(\mu_z, \sigma_z)$}}.
\end{definition}

This definition encompasses more specific definitions of related
work~\cite{hsiao2005scoped, lue2006scoped, wang2015prodiluvian}.  It
also highlights that epidemic propagation and scoped broadcast have
well-aligned preoccupations. More precisely, it underlines the transitive relevance of
messages, thus \processes can stop forwarding messages as soon as the
corresponding predicate becomes unverified.
%For instance, a \process
%from Paris could scoped broadcast messages to all \processes in
%Paris. This requires \processes to store and maintain their city
%location in their local state. \Processes stop delivering and
%forwarding their received messages when they come from a different
%city.  In another instance, a \process from Paris could scoped
%broadcast messages to all \processes in Paris plus neighboring
%cities. This requires \processes to overload forwarded messages the
%first time they reach another city. The predicate checks if messages
%already reached two distinct cities before delivery. Similarly to
%uniform reliable broadcast, scoped broadcast implementations expose
%different trade-offs on space, time, and communication.
%
We use \NAMEB to dynamically and efficiently modify the state of each
\process depending on all partitions that exist in the system.



\subsection{Consistent partitioning}
\label{subsec:consistent}

At any time, a \process can decide to become a \emph{source}, hence
creating a new partition in the system by executing an \texttt{Add}
operation. This partition includes at least its source plus
neighboring \processes that estimate they are closer to this source
than any other one. Such distance (or \emph{weight}) is
application-dependant: in the context of maintaining distributed
indexes, this would be about link latency.

\begin{definition}[\label{def:partitioning}Partitioning]
  Let $S \subseteq V$ be the set of \underline{s}ources, and $P_s$ be
  the \underline{p}artition including at least \Process~$s$, each
  \process belongs to at most one partition \smash{\small$\forall p,q \in V, \forall
  s_1,s_2 \in S: p \in P_{s_1} \wedge q \in P_{s_2} \implies p \neq q
  \vee s_1 = s_2$}, and there exists at least one path $\pi_{ps}$ of
  \processes that belong to this partition $\forall q \in \pi_{ps}: q
  \in P_s$.
\end{definition}

Definition~\ref{def:scoped} and Definition~\ref{def:partitioning}
share the transitive relevance of \process states. However, we further
constrain the partitioning in order to guarantee the existence of
exactly one consistent partitioning that \processes eventually converge
to.

\begin{definition}[\underline{C}onsistent \underline{p}artitioning (CP)]
  Let $W_{xy} = W_{yx}$ be the positive symmetric \underline{w}eight
  between $x$ and $y$, $\Pi_{xz}$ be the shortest \underline{p}ath
  from $x$ to $z$ the weight of which $|\Pi_{xz}|$ is lower than any
  other path weight, the only consistent partitioning $\mathcal{P}$ is
  a set of partitions $P_{s\in S}$ such that each \process belongs to
  a logical partition comprising its closest source: $\forall p \in
  P_{s_1}: \nexists P_{s_2}$ such that $|\Pi_{s_1p}| > |\Pi_{s_2p}|$.
\end{definition}

Unfortunately, \processes do not share a common global knowledge of
the network state. For \processes to reach consistent partitioning
together, a \Process $s$ \underline{a}dding a partition must send a
notification $\alpha_s$ to every \process that is closer to it than
any other source. Since epidemic dissemination and scoped broadcast
have well-aligned preoccupations, we assume implementations relying on
message forwarding from neighbor-to-neighbor.

\begin{theorem}[\label{theo:bef}\underline{B}est \underline{e}ventual
    \underline{f}orwarding $(BEF) \implies CP$]
%
Assuming reliable communication links where a correct \process $q$
eventually receives the message $m$ \underline{s}ent to it by a
\process $p$ ($s_{pq}(m) \implies r_{q}(m)$), \processes eventually
reach consistent partitioning if each \process eventually forwards its
best known partition.
\end{theorem}

\input{input/figadd.tex} 

\begin{proof}
  % \begin{asparadesc}
  % \item [$BEF \implies CP$:]
  When a \process $s_1$ becomes a source, it belongs to its own
  partition, for there exists no better partition than its own:
  $\forall p \in V: |\Pi_{s_1 s_1}| < |\Pi_{s_1 p}|$. It delivers,
  hence forwards such an $\alpha$ message to its neighbors. Since
  communication links are reliable, neighboring \processes eventually
  receive such a notification $\forall \langle s_1, q \rangle \in E,
  s_1 \in S \iff \eventually r_q(\alpha_{s_1}^{w_{s_1 q}})$. Most
  importantly, whatever the order of received messages, every \process
  $q'$ in this neighborhood -- such that there exists no better
  partition $s_2$ than the received one -- delivers and forwards it:
  $\forall s_2 \in S: |\Pi_{q' s_1}| < |\Pi_{q' s_2}| \implies
  \eventually d_{q'}(\alpha_{s_1}^{w_{s_1 q'}})$. By transitivity, the
  message originating from $s_1$ reaches all such \processes through
  their shortest paths: $\forall q'' \in V, s_1, s_2 \in S: |\Pi_{q''
    s_1}| < |\Pi_{q'' s_2}| \implies \eventually
  d_{q''}(\alpha_{s_1}^{|\Pi_{s_1 q''}|})$.  Since there exists only
  one best sum of weights per \process that can never be retracted,
  \processes eventually reach consistent partitioning.
  %% /!\ not equivalence for there exists other implementations.
  %% \item [$CP \implies BEF$:] By contradiction, if a \process $q \in
  %%   \Pi_{sp} = [s, \ldots, q, q', \ldots, p]$ with $s, q, p \in P_s$
  %%   does not forward its received $\alpha_s^{|\Pi_{sq}|}$, then
  %%   following \processes from $q'$ to $p$ may mistake another
  %%   partition for their  because it needs the weight $W_{pq}$.
  %% \end{asparadesc}
\end{proof}

\begin{algorithm}
  \input{input/algoadd.tex}
  \caption{\label{algo:add}Adding a partition by \Process~$p$.}
\end{algorithm}

Algorithm~\ref{algo:add} shows the instructions that implement
consistent partitioning when
\begin{inparaenum}[(i)]
\item weights are scalar values,
\item \processes only add new partitions to the system,
\item and \processes never crash nor leave the system.
\end{inparaenum}
Figure~\ref{fig:add} illustrates its behavior on a system comprising 4
\processes $a$, $b$, $c$, and $d$. \Process~$a$ and \Process~$d$
become the sources of their partition. They \NAMEB a notification
\underline{a}dd message: $\alpha_a^0$ and $\alpha_d^0$. They
initialize their own state with the lowest possible bound $0$ (see
Line~\ref{line:lowestbound}), and send a message to each of their
neighbors by accumulating the corresponding edge weight (see
Line~\ref{line:accumulator}). In Figure~\ref{fig:addC}, \Process~$b$
receives $\alpha_{d}^{1}$. Since it improves its own partition
distance, it keeps it and forwards it to its neighbors. In
Figure~\ref{fig:addD}, \Process~$b$ discards $\alpha_{a}^{2}$, for it
does not improve its partition distance. \Processes $c$ and $d$ will
never acknowledge that Source~$a$ exists. Ultimately, \processes
discard last transiting messages. Despite the obvious lack of traffic
optimization, the system reaches consistent partitioning.

While only adding logical partitions to the distributed system is
straightforward, removing them introduces additional complexity.

\subsection{Dynamic consistent partitioning}
\label{subsec:dynamic}

%% At any time, a \process can become a source, hence adding a new
%% partition to the system. This partition eventually includes all
%% \processes that are closer from this new source than any other
%% else. \Processes naturally converge towards their respective best
%% partition by only piggybacking a monotonically increasing distance in
%% forwarded messages.

At any time, a source can revoke its self-appointed status of source
by executing a \texttt{Del} operation, hence deleting its partition
from the system. All \processes that belong to this partition must
eventually choose another partition to belong to. In
Figure~\ref{fig:del}, two partitions exist initially: $P_a$ and $P_d$
that respectively include $\{a\}$, and $\{b, c, d\}$. In
Figure~\ref{fig:delA}, \Process~$a$ deletes its partition. It notifies
all neighboring \processes~--~here only \Process~$b$~-- that may
belong to its partition using \NAMEB. Upon receipt, \Process~$b$
discards the \underline{d}elete notification $\delta_a$ since
$\delta_a$ does not target the former's partition $P_d$. \Process~$b$
sends its own best partition $\alpha_d^3$ that may be the best for
\Process~$a$. Eventually, every \process belongs to Partition
$P_d$. In this scenario, they reach consistent partitioning.

Delete operations add a new notion of order between events, and most
importantly between message deliveries. Delete operations implicitly
state that all preceding events become obsolete, and that all messages
originating from these preceding events convey stale control
information.

\begin{definition}[Happens-before $\rightarrow$~\cite{lamport1978time}]
  The transitive, irreflexive, and antisymmetric happens-before
  relationship defines a strict partial order between events. Two
  messages are concurrent if none happens before the other.
\end{definition}

\begin{definition}[\label{def:lwo}Last win order and staleness]
  When a \process $p$ broadcasts two messages $b_p(m) \rightarrow
  b_p(m')$, no \process $q$ can deliver $m$ if it has delivered $m'$:
  $\not\exists q \in V$ with $d_q(m') \rightarrow d_q(m)$, for $m$
  convey \emph{stale} control information.
\end{definition}

\input{input/figdel.tex}

\input{input/figproblem.tex}


Unfortunately, stale control information as stated in
Definition~\ref{def:lwo} may impair the propagation of both
\begin{inparaenum}[(i)]
\item notifications about actual sources, and
\item notifications about deleted partitions.
\end{inparaenum}
Figure~\ref{fig:problem} highlights such consistency issues with
dynamic partitions, even in contexts where \processes have FIFO links,
\ie where \processes receive the messages in the order of their
sending. Both \Process~$a$ and \Process~$d$ add then delete their
partition concurrently. \Process~$c$ receives, delivers, and forwards
$\alpha_d^3$ followed by $\delta_d$. In the meantime, \Process~$b$
receives, delivers, and forwards $\alpha_a^2$. In
Figure~\ref{fig:problemC}, both \Process~$c$ and \Process~$d$ deliver
the message about Partition $P_a$, for they did not have any known
partition at receipt time. On the contrary, \Process~$b$ delivers
$\alpha_d^1$, for it improves its distance to a known
source. \Process~$b$ then blocks \Process~$a$'s removal notification
$\delta_a$. It never reaches \Process~$c$ nor \Process~$d$. Also,
\Process~$c$ does not deliver $\alpha_d$ and $\delta_d$ since it
already delivered it. The system converges to an inconsistent state
where some \processes assume they belong to a partition while it does
not exist anymore. Naive propagation of $\alpha$ and $\delta$ messages
is insufficient to guarantee consistent partitioning when any \process
can add or delete its partition at any time. \Processes need to
exploit their local knowledge gathered over message receipts.


One could guarantee consistent partitioning by always propagating the
$\delta$ messages corresponding to the $\alpha$ messages it propagated
before. In Figure~\ref{fig:problem}, it means that as soon as
\Process~$b$ forwards $\alpha_a^2$, it assumes that its neighbors
\Process~$c$ and \Process~$d$ may need the notification of removal
$\delta_a$ if it exists. However, such a solution also implies that
\processes generate traffic not only related to their current
partition, but also related to partitions they belong to in the
past. This would prove overly expensive in dynamic systems where
\processes join and leave the system, create and delete partitions, at
any time.  Instead, we propose to use the delivery order at each
\process to detect possible inconsistencies and solve them. Together,
\processes eventually remove all stale control information (transiting
messages and local states) of the system leaving room for propagation
of messages about up-to-date partitions.

\begin{corollary}[\label{theo:dcp}purge + BEF $\implies$
    \underline{D}ynamic \underline{c}onsistent
    \underline{p}artitioning (DCP)]
%
When each \process can \texttt{Add} and \texttt{Del}ete its partition
at any time, consistent partitioning requires
\begin{inparaenum}[(i)]
\item eventual purging of stale messages ($b_p(m) \rightarrow b_p(m')
  \implies \eventually (d_q(m) \implies d_q(m) \rightarrow
  d_q(m''))$);
\item and best eventual forwarding.
\end{inparaenum}
\end{corollary}

\begin{proof}
  \Processes deliver and forward the best sum of weights they
  received. Figure~\ref{fig:proofA} depicts the only case where a
  \process would block the epidemic propagation: the \process received
  and delivered a message from a partition that has since then been
  deleted but with a better sum of weights than newly received ones
  (in the example $\alpha_d^y < \alpha_a^{x'}$). Removing
  \emph{forever} all such stale messages about deleted partitions
  would allow \processes to propagate their best up-to-date partition
  again, eventually reaching consistent partitioning together, as
  stated by Theorem~\ref{theo:bef}.
\end{proof}

\input{input/figproof.tex}

%% Figure~\ref{fig:proof} depicts the issues with staleness and message
%% orderings. In Figure~\ref{fig:proofA}, the shortest path from any
%% source to \Process $c$ is $[a, b, c]$. However, \Process $b$ still holds
%% a stale $\alpha_d^{0.5}$ without knowing. When it receives
%% $\alpha_a^1$, it discards it, for it assumes that downstream \processes
%% are more interested in $\alpha_d^{0.5}$. To reach consistent
%% partitioning, \Process $b$ first needs to purge its current partition
%% to later accept that of its current actual shortest path:
%% $\alpha_a^1$.

%% \noindent Figure~\ref{fig:proofB} shows that removing stale control
%% information is even more complex. The removal must reach all \processes
%% of the previous shortest path going from $d$ to $b''$. Label I' shows
%% the most obvious issue where \Process $e$ changed partition for a
%% better but stale $\alpha_f$. Since it can remember its previous
%% deliveries, it could still forward $\delta_d$ for the sake of
%% consistency. However, this would lead to every \process forwarding
%% every $\delta$ they ever delivered. Such protocol's overhead would
%% depend on past partitioning instead of current one. Label III shows
%% the issue when the blocking partition is already known to be stale at
%% \Process $b'$. \Process $b''$ eventually receives $\alpha_f$ from
%% $e$. However, it cannot deliver it, for it would break last win order.
%% \Process $b'$ may be in an inconsistent state. Label IV shows the
%% corollary issue: \Process $b''$ delivered a message from a \process that
%% may be inconsistent without knowing it.

\begin{theorem}[three-phase purge] % Propagations $+$ detection $\implies$ purge]
  Purging stale control information from its source to every \process
  that may have delivered it last requires three
  phases:
  \begin{inparaenum}[(i)]
  \item propagation of delete notifications,
  \item detection of possible blocking conditions, and
  \item propagation of possibly deleted but blocked notifications.
  \end{inparaenum}
\end{theorem}

\begin{proof}
  Figure~\ref{fig:proofB} depicts all possible cases that need
  appropriate purging. A chain of \processes $[d, e, f, g]$ delivered
  and forwarded a message $\alpha_d$. Some \processes may have
  delivered messages about other partitions before or after
  $\alpha_d$.
  \begin{asparadesc}
  \item [\processes with last $\alpha_d$:] \Processes such as
    \Process~$d$ where the last and best partition is still $P_d$. By
    propagating from \process to \process the corresponding $\delta_d$
    message, these \processes purge $\alpha_d$.
  \item [\processes with $\alpha_d^y \rightarrow \alpha_h^z$ with last
    $\alpha_h^z$, for $\alpha_h^z < \alpha_d^y$:] \Processes such as
    \Process~$e$ that do not directly suffer from the non-delivery of
    $\delta_d$. They need to purge their own $P_h$ if need
    be. Nonetheless, they deliver and forward $\alpha_h$ that at least
    one following \process will receive:
  \item [\processes with $\delta_h \rightarrow \alpha_d$:] \Processes
    such as \Process~$f$ that do not belong to the preceding category,
    for they already delivered $\delta_h$. Nevertheless, best eventual
    forwarding guarantees that they receive $\alpha_h$, which
    contradicts their history or state. In such a case, the detecting
    \processes purge their own $\alpha_d$. The detecting \processes
    must also notify subsequent \processes that their current
    partition \emph{may be} deleted. Indeed their parent may have
    blocked the corresponding $\delta$ messages and subsequent
    \processes must purge their possibly stale state, for the sake of
    consistency.  We note such notification $\delta_d?$ to emphasize
    the uncertainty of a $\delta$ message.
  \item [\processes with last $\alpha_d$ receiving $\delta_d?$ from
    their parent:] \Processes such as \Process~$g$ that have
    $\alpha_d$ but preceding delivered messages are not
    important. Such \processes must trust detecting \processes by
    delivering and forwarding $\delta_d?$. It is worth noting that
    these messages are subject to all aforementioned blocking
    conditions, but are also solved by aforementioned mechanisms.
  \end{asparadesc}

  Propagation~I' terminates: a \process does not deliver a $\delta$
  message if it has already delivered it. Propagation~IV terminates: a
  \process does not deliver a looping message.
  % ; and since \processes forward the messages they deliver, upstream
  % \processes eventually receive and may deliver messages detected as
  % issue. In Figure~\ref{fig:proofB}, \Process~$e$ eventually
  % receives $\delta_f$ from \Process~$b'$.
  These three phases propagation-detection-propagation ensure that
  \processes eventually purge stale $\alpha$ messages from the system,
  leaving room for Propagation~I.
  %
  %  \TODO{cannot loop add from $e$ , undo from $b'$ because $b'$ sent
  % delete to $e$, it will solve the inconsistency.}
  %
  %% In the meantime, \processes that still belong to a partition can
  %% propagate their respective last $\alpha$ message, and reach
  %% consistent partitioning.
\end{proof}



% \vfill

% \TODO{spacing.}

% \newpage

\subsection{Implementation and complexity}

Algorithm~\ref{algo:ascast} provides the few instructions of \NAME
that implement three-phase purge and best eventual forwarding to
enable dynamic consistent partitioning.

%% Similarly to Algorithm~\ref{algo:add}, local operations \texttt{Add}
%% and \texttt{Del} generate messages that the initiating \process treats
%% identically to receipts of messages from remote \processes.

%% \begin{inparaenum}[(i)]
%% \item \label{algo:most} most of the time, it propagates $\alpha$'s and
%%   $\delta$'s when the partitions allow it;
%% \item \label{algo:sometimes} sometimes, it detects possible
%%   inconsistent partitioning and
%% \item \label{algo:solves}solves it using propagation trees (as opposed
%%   to propagation graphs);
%% \item \label{algo:competition} when required, it triggers competitions
%%   among neighboring partitions.
%% \end{inparaenum}
%% While (\ref{algo:most})-(\ref{algo:solves}) tackle the eventual purging of
%% stale notifications, (\ref{algo:competition}) tackle the eventual
%% delivery of the best up-to-date partitions.

\begin{algorithm}
  \input{input/algoascast.tex}
  \caption{\label{algo:ascast}\NAME at \Process~$p$ in static networks.}
\end{algorithm}

To implement last win order as stated in Definition~\ref{def:lwo},
each \process maintains a vector of versions that associates the
respective known local counter of each known source, or has-been
source. Each message $m$ carries its source $s$ along with its version
$c$ that we subscript $m_{s, c}$.  Each \process delivers and forwards
\begin{inparaenum}[(i)]
\item only newest messages (see Line~\ref{line:ascast_version}) that
\item improve their best known partition (see
  Line~\ref{line:ascast_better}).
\end{inparaenum}
This vector enables both Propagation~I and Propagation~I'.  Its size
increases linearly with the number of sources that the \process ever
known, which is the number of \processes in the system
$\mathcal{O}(V)$ in the worst case.
%% Unfortunately, \processes cannot
%% remove any entry from this vector without running a costly distributed
%% consensus protocol to ensure that no \process can mistake an old but
%% removed entry for a new one.
Nevertheless, following the principles of scoped broadcast, we expect
that every \process only acknowledges a small subset of sources.

This vector of versions constitutes a summary of deliveries. \NAME
uses it to detect possible inconsistent partitioning as depicted in
Figure~\ref{fig:proofB}. A detecting \process then propagates a
$\delta$ message conveying the unique identifier $s$ and version $c$
of the partition along with a path $\pi$ comprising the identity of
\processes that forwarded the corresponding $\alpha$ message. Such
$\delta_{s, c}^{\pi}$ message implements the uncertain $\delta?$
message of Figure~\ref{fig:proofB}. It fulfils the double role of
removing stale control information without modifying the local counter
of the source (since the partition may still exist), and removing
loops to ensure termination (since \processes do not modify their
local state to reflect such a delivery, messages piggyback
monotonically such increasing control information).

$\delta_{s, c}$ messages used in Propagation~I' are more efficient
than $\delta_{s, c}^\pi$ messages used in Propagation~IV, both in
propagation time and traffic. First, the former propagate using every
communication link available in the partition, while the latter
propagate following the dissemination tree of the corresponding
$\alpha$ to purge. Second, the former only piggyback source and
counter while the latter piggyback the path taken by the corresponding
$\alpha$ to purge.  In the worst case, a path includes every \process
in the system but converges to the average diameter of
partitions. Nevertheless, message paths and \NAME synergyze with each
other: Paths tend to be smaller as the number of sources in the system
increases. Bloom filters could introduce another trade-off by further
decreasing the size of path data structures at the cost of premature
stopped propagation that could impair consistent
partitioning~\cite{whitaker2002forwarding}.

As stated in Theorem~\ref{theo:dcp}, dynamic consistent partitioning
not only requires eventual purging of stale control information, but
also the retrieval of the best up-to-date partitions. In that regard,
both kinds of $\delta$ messages have dual use: since they already
reach the borders of their partition when they remove stale control
information, they also trigger a competition when reaching such
bordering \processes (see Line~\ref{line:ascast_compete}). This simply
consists in sending the current partition through the communication
link from which the \process received the $\delta$. Upon receipt of
this answer, \processes act normally by propagating their changes when
they improve.

\NAME guarantees dynamic consistent partitioning by making extensive
use of \process-to-\process communications. It requires
\begin{inparaenum}[(i)]
\item purging stale control information, hence propagating $\delta$
  messages; and
\item retrieving the closest source, hence propagating $\alpha$
  messages.
\end{inparaenum}
In terms of number of messages, in the average-case, a \process $i$
chosen uniformly at random among all \processes creates a logical
partition. Its messages $\alpha_i$ spread through the network until
reaching \processes that belong to another partition closer to
them. This splits partitions in half in average. Overall, the $a^{th}$
new partition comprises \smash{$\mathcal{O}(\frac{|V|}{2^{\lfloor
      \log_2 a \rfloor}})$} \processes. This decreases every new
partition until reaching one \process per new partition: its
source. The average number of messages per \process is
\smash{$\mathcal{O}(\frac{\overline{|O|}}{2^{\lfloor \log_2 a
      \rfloor}})$}, where \smash{$\overline{|O|}$} is the average
number of neighbors per \process. % \TODO{Multiple receipt and
multiple
%  delivery imply more messages (receipt bounded by $|O|$ as well).}
Deleting the $a^{th}$ partition generates twice as many messages as
creating the $a^{th}$ partition: $\delta$ messages travel through the
partition, then $\alpha$ messages compete to fill the gap.  Overall,
the communication complexity shows that \NAME scales well to the
number of partitions.
%% In the
%% worst-case, every new partition includes all but one \process
%% belonging to the previous partition. The total number of messages
%% after the $a^{th}$ new partition is $\mathcal{O}(\overline{|O|}\cdot
%% a^2)$. As for the average-case, the number of messages for the
%% partition deletion is identical to the number of messages of the
%% corresponding partition creation.

\begin{algorithm}
  \input{input/algoedges.tex}
  \caption{\label{algo:edges}\NAME at \Process~$p$ in dynamic networks.}
\end{algorithm}

Algorithm~\ref{algo:edges} extends \NAME to enable dynamic consistent
partitioning in dynamic networks where \processes can join or leave
the system without notification. We consider that removing a \process
is equivalent to removing all its edges, and adding a \process is
equivalent to add as many edges as necessary.

Adding an edge between two \processes may only improve the shortest
path of one of these \processes. Therefore, it triggers a competition
between the two \processes only. If one or the other \process
improves, the propagation of $\alpha$ messages operates normally.
Removing an edge between two \processes may invalidate the
shortest path of one of involved \processes plus downstream
\processes. As a side effect, removing an edge may also impair the
propagation of a partition delete. To implement this behavior, \NAME
uses its implementation of $\delta?$ messages. This prove costly but
enables \NAME even in dynamic networks subject to physical
partitioning.

Next Section provides the details of our simulations that assess the
proposed implementation.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 
