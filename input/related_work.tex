
\section{Related Work}
\label{sec:related_work}


\begin{asparadesc}
\item [Remote third-party:]
Relying on a remote third party service is a popular approach to offer content indexing in geo-distributed infrastructures. 
Such a remote service should
\begin{inparaenum}[(i)]
\item have access to the list of current replica locations and
\item be aware of the topology to decide which replica is the
  closest for a given location.
\end{inparaenum}

\begin{inparaenum}[(i)]
\item Numerous solutions~\cite{snamp, p2p-oracle, fogstore, p2p-alto} maintain an index of all replica
locations on a centralized server. This comes with well-known disadvantages
inherent to centralized solutions such as load balancing, robustness or locality that are even magnified in an Edge context.
Distributing this index among nodes in the network circumvent some of
these issues, either using coordinate-based overlays~\cite{voronet,
  coin_19} or \underline{D}istributed \underline{H}ash
\underline{T}ables (DHTs)~\cite{ipfs, mdht, squirrel}. In
the latter, each node stores a part of the index, defined by an
interval between hash values.  Before downloading any object, a node
first hashes the content to obtain the address of the node that stores
the replica locations of this given object. After obtaining from the
remote node the list of available replicas, it can select the closest
copy to download from. We emphasize that, in this setting, the DHT
only stores information about replicas locations, not the content
itself. 


\item Determining where resides the closest replica necessarily involves some knowledge about the current topology.
Maintaining a consistent view of an ever changing
topology across a network is inherently complicated, especially in
asynchronous settings. As for the index,
this knowledge about the topology can be either
centralized~\cite{topology-discovery} or distributed like in routing
protocols such as \underline{O}pen \underline{S}hortest
\underline{P}ath \underline{F}irst (OSPF)~\cite{ospf}.
\end{inparaenum}


All the aforementionned solutions require to contact a remote node to request this index which is costly in terms of
latency while in contradiction with Edge infrastructure constraints, as previously explained.


\item [Broadcast:]
The opposite approach consists in hosting the index directly on the
nodes, avoiding any prior request to access a content.
\underline{N}amed \underline{D}ata \underline{N}etworking
(NDN)~\cite{nlsr} broadcasts information about cache updates to all nodes in the
system. Having the entire knowledge of all the
replica locations along with distance information carried into
messages, each node can easily determine where the closest copy of a
given object resides, without contacting any remote node.  Similarly,
one could use a
\underline{C}onflict-\underline{F}ree \underline{R}eplicated
\underline{D}atatype (CRDT) for set data structures, for
example~\cite{shapiro2011crdts}. Yet both solutions inherently imply
that every node will eventually receive all messages, contrary to \NAME.


\item [Membership protocols:]
At first glance, membership protocols~\cite{t-man}
would appear to provide suitable abstractions to create a
scope-related partitioning, as proposed in this paper. Such protocols
usually rely on gossip mechanisms, implying contacts with random nodes
that can be far away in the physical topology. 
They are mostly implemented by modifying the
neighboring of a given node according to a certain metric in a logical
overlay. On the contrary,  \NAME  does not aim at building any overlay, as
it only relies on the physical topology so the neighbors of a given node
are not continuously modified. These protocols are usually
cycle-based, meaning that they impose a constant load on the network
traffic, even when the system state does not change. \NAME only generates traffic upon system changes.

\item [Timeouts:]
Solutions that periodically advertise their
cache content~\cite{garcia-lopez, hemmati2015namebased} also
incur an overhead even in quiet contexts without any update.
Moreover, they necessarily rely on physical timeouts that
could lead to either premature removals of partitions when they are
still operating; or slow convergence where processes wrongly believe
they belong to a partition that was removed. On the opposite,
Section~\ref{sec:experimentation} shows that \NAME quickly converges
to consistent partitioning even in large scale networks.

\end{asparadesc}

%Finally, we emphasize that consistency is at the core of our design,
%as \NAME provides the same guarantees for all nodes.  On the
%contrary, to avoid flooding, authors in~\cite{opnl} propose to
%propagate information about cache updates only towards the original
%replica. This enables for a request to be caught \textit{on-path},
%and be redirected to the closest replica. However, the announcement
%is only performed towards this original replica, that is requests can
%miss closer replicas that are not on this path, thus decreasing the
%efficiency of the cache sharing mechanism.

%AVK: This should go where we explain the DEL, as this is really
%specific...  One could also solve this issue by removing partitions
%that were not advertised for a defined
%time~\cite{hemmati2015namebased, garcia-lopez}.  However, relying on
%physical timeout could lead to either premature removals of
%partitions when they are still operating; or slow convergence where
%processes believe they belong to a partition that was removed
%(\TODO{plot ?}). In addition, since timeouts imply a continuous
%maintenance of partitions, such partitioning protocol incurs an
%overhead even in quiet contexts without dynamic partitions.


% AVK t-man: T-Man: Gossip-Based Overlay Topology Management5doat a
% random time at a random time once in eachconsecutive interval of T
% time unit This buffer contains a random sample of the nodes from the
% entirenetwork. It is provided by apeer sampling service We do not
% try to to create a topology aware overlay ! Because locality.  We do
% not provide routing ! routing is two determine path between any pair
% of nodes. We only provide path to objects Koala locality aware
% routing. Long links. Lazzy (nothing happens) Koala shares many core
% aspectswith similar overlays, such as Chord and Symphony.  VoroNet:
% overlay and routing. i.e. long links It links application objects
% rathen than physical nodes so that objects with similar
% characteristic are neighbors.  are logigal overlay, that are not
% correlated to the physical infrstructure.  Usually rely on long
% links...  AVK

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 

