
\section{Related Work}
\label{sec:related_work}

Content indexing services in geo-distributed infrastructures allow
\processes to retrieve specific content while leveraging the
advantages of replication. These systems mostly rely on dedicated
location services hosted by possibly remote third-party \processes; but
cutting out the middleman requires that each \process maintains its
own index in a decentralized fashion.

\begin{asparadesc}
\item [Third-party:]

Dedicated services possibly hosted by remote third-party \processes
are popular, for they propose the simplest mean to deploy such a
service. They must maintain
\begin{inparaenum}[(i)]
\item the list of current replicas along with their respective
  location; and
\item the network topology to determine the closest replica for every
  requesting \process.
\end{inparaenum}

\noindent A central server that registers both these information
facilitates the computation, for it gathers all knowledge in one
place~\cite{snamp, p2p-oracle, fogstore, p2p-alto}. However, it comes with
well-known issues such as load balancing, or single point of failure.
%% Numerous solutions
%% maintain an index of all replica locations on a centralized
%% server. This comes with well-known disadvantages inherent to
%% centralized solutions such as load balancing, robustness or locality
%% that are even magnified in an Edge context.

\noindent Distributing this index among \processes circumvents these
issues~\cite{voronet, ipfs, mdht, squirrel, triukose2011measuring,
  coin_19}, but still raises locality issues where \processes
\begin{inparaenum}[(i)]
\item request to possibly far away location services the content
  location, and then
\item request the actual content from possibly far away replicas.
\end{inparaenum}
For instance, using \underline{d}istributed \underline{h}ash
\underline{t}ables (DHT)~\cite{ipfs, mdht, squirrel}, each \process
stores a part of the index defined by an interval between hash values.
Hash values are keys to retrieve the associated content location.
Before downloading any content, a \node requests its location using
its key. After round-trips between possibly distant DHT servers, the
\node gets available replicas. Contrarily to \NAME, such services do
not ensure to include the closest replica.

\input{input/tablerelatedwork.tex}

\noindent In addition, they often do not include distance information
along with replica location. Determining where resides the closest
replica for every requesting \process necessarily involves some
knowledge about the \emph{current} topology.  Maintaining a consistent
view of an ever changing topology across a network is inherently
complicated~\cite{topology-discovery, ospf}.
%%  As for the index, this knowledge about the topology can be either
%% centralized~\cite{topology-discovery} or distributed like in
%% routing protocols such as \underline{O}pen \underline{S}hortest
%% \underline{P}ath \underline{F}irst (OSPF)~\cite{ospf}.
As a consequence, the requesting \node ends up downloading from multiple
replica hosts, yet only keeping the fastest answer. \Processes either
waste network resources, or face lower response time.

% We emphasize that, in this setting, the DHT only stores information
% about replicas locations, not the content itself.

%% Distributing this index among \processes in the network circumvent
%% some of these issues, either using coordinate-based
%% overlays~\cite{voronet, coin_19} or \underline{D}istributed
%% \underline{H}ash \underline{T}ables (DHTs)~\cite{ipfs, mdht,
%%   squirrel}. In the latter, each node stores a part of the index,
%% defined by an interval between hash values.  Before downloading any
%% object, a node first hashes the content to obtain the address of the
%% node that stores the replica locations of this given object. After
%% obtaining from the remote node the list of available replicas, it can
%% select the closest copy to download from. We emphasize that, in this
%% setting, the DHT only stores information about replicas locations, not
%% the content itself.



%% \noindent All the aforementionned solutions require to contact a remote node to
%% request this index which is costly in terms of latency while in
%% contradiction with Edge infrastructure constraints, as previously
%% explained.


\item [Fully decentralized:]

Cutting out the middleman by hosting a location service on each and
every \process improves response time of content retrieval. However,
it still requires every \process to index every live replica as well
as their respective distance in order to rank them.
%% The opposite approach consists in hosting the index directly on the
%% nodes, avoiding any prior request to access a content.
\underline{N}amed \underline{D}ata \underline{N}etworking
(NDN)~\cite{nlsr} broadcasts information about cache updates to all
\nodes in the system. Having the entire knowledge of all the replica
locations along with distance information carried into messages, each
\node easily determine where the closest copy of specific content
resides, without contacting any remote
service. In a routing context, distance-vector routing protocols such as BGP or OSPF~\cite{ospf} similarly broadcast information to all other nodes to infer a topology map of the network and derive routing tables.
\underline{C}onflict-\underline{f}ree \underline{r}eplicated
\underline{d}atatype (CRDT)~\cite{shapiro2011crdts} for set data
structures could also implement such a location service. \Processes
would eventually have a set of all replicas, assuming eventual
delivery of messages.
\noindent Such solutions imply that every \process receives every
message, which contradicts the principles of Edge infrastructures that
aim at reducing the volume of data passing through the network. On the
opposite, each \process running \NAME only registers its closest
replica. This allows \NAME to use scoped broadcast as a communication
primitive to lock down the traffic generated by content indexing based
on distances.
% Such solutions inherently imply that every node will
% eventually receive all messages, contrary to \NAME.

\item [Scoped flooding:]
  Some approaches also confine the dissemination of messages.
\noindent Distance-based membership protocols such as
T-Man~\cite{t-man} make \processes converge to a specific network
topology depending on targeted properties. Following periodic
exchanges, they add and remove communication links to other \processes
to converge towards a topology ensuring the targeted properties.
While membership protocols and \NAME share common preoccupations,
\NAME does not aim at building any topology and never modifies
neighbors of \processes.

%% In
%% addition, distance-based membership protocols often rely on random
%% peer-sampling protocols to avoid falling in a local minimum that would
%% stop them from reaching this targeted topology. This contradicts with
%% the principles of Edge infrastructures as well, in a minor way.

%% At first glance, membership protocols~\cite{t-man} would appear to
%% provide suitable abstractions to create a scope-related
%% partitioning, as proposed in this paper. Such protocols usually
%% rely on gossip mechanisms, implying contacts with random nodes that
%% can be far away in the physical topology.  They are mostly
%% implemented by modifying the neighboring of a given node according
%% to a certain metric in a logical overlay.

%% These protocols are usually cycle-based, meaning that they impose a
%% constant load on the network traffic, even when the system state does
%% not change. \NAME only generates traffic upon system changes.

\noindent The most closely related approaches to \NAME come from
\underline{i}nformation-\underline{c}entric \underline{n}etworking
(ICN)~\cite{garcia-lopez, hemmati2015namebased} and distributed
clustering~\cite{sohier2012physarum}. The sources advertise themselves
periodically. Advertisements either stop as soon as they reach
uninterested \processes~\cite{garcia-lopez, hemmati2015namebased}, or
propagate through most likely interested
\processes~\cite{sohier2012physarum}. However their operation requires
that
\begin{inparaenum}[(i)]
\item they constantly generate traffic even in quiescent systems where
  \processes do not add nor destroy replicas; and 
\item they either operate in synchronous
  systems~\cite{sohier2012physarum} or must define
  physical-clock-based timeouts the value of which depends on network
  topology properties such as network diameter~\cite{garcia-lopez,
    hemmati2015namebased}.
  %also incur an overhead even in quiet contexts without any update.
  % Moreover, they necessarily rely on physical timeouts
  Unfitting timeouts lead to premature removals of live replicas; or
  slow convergence where \processes wrongly believe that their closest
  replica is live while it was destroyed.
\end{inparaenum}
On the opposite, \NAME quickly informs each \process of its closest
replica even in large and dynamic networks with asynchronous
communications; and has no overhead when the system is quiescent.

\end{asparadesc}

%Finally, we emphasize that consistency is at the core of our design,
%as \NAME provides the same guarantees for all nodes.  On the
%contrary, to avoid flooding, authors in~\cite{opnl} propose to
%propagate information about cache updates only towards the original
%replica. This enables for a request to be caught \textit{on-path},
%and be redirected to the closest replica. However, the announcement
%is only performed towards this original replica, that is requests can
%miss closer replicas that are not on this path, thus decreasing the
%efficiency of the cache sharing mechanism.

%AVK: This should go where we explain the DEL, as this is really
%specific...  One could also solve this issue by removing partitions
%that were not advertised for a defined
%time~\cite{hemmati2015namebased, garcia-lopez}.  However, relying on
%physical timeout could lead to either premature removals of
%partitions when they are still operating; or slow convergence where
%processes believe they belong to a partition that was removed
%(\TODO{plot ?}). In addition, since timeouts imply a continuous
%maintenance of partitions, such partitioning protocol incurs an
%overhead even in quiet contexts without dynamic partitions.


% AVK t-man: T-Man: Gossip-Based Overlay Topology Management5doat a
% random time at a random time once in eachconsecutive interval of T
% time unit This buffer contains a random sample of the nodes from the
% entirenetwork. It is provided by apeer sampling service We do not
% try to to create a topology aware overlay ! Because locality.  We do
% not provide routing ! routing is two determine path between any pair
% of nodes. We only provide path to objects Koala locality aware
% routing. Long links. Lazzy (nothing happens) Koala shares many core
% aspectswith similar overlays, such as Chord and Symphony.  VoroNet:
% overlay and routing. i.e. long links It links application objects
% rathen than physical nodes so that objects with similar
% characteristic are neighbors.  are logigal overlay, that are not
% correlated to the physical infrstructure.  Usually rely on long
% links...  AVK

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 

