
\section{Related Work}
\label{sec:related_work}

Content indexing services in geo-distributed infrastructures allow
\processes to retrieve specific content while leveraging the
advantages of replication. These systems mostly rely on dedicated
location services host by possibly remote third-party \processes~\REF;
and cutting the middleman requires that each \process maintains its
own index in a decentralized fashion~\REF. This section reviews these
approaches. 

\begin{asparadesc}
\item [Third-party:]

Dedicated services possibly hosted by remote third-party \processes
are popular, for they propose the simplest mean to deploy such
service. They must maintain
\begin{inparaenum}[(i)]
\item the list of current replicas along with their respective
  location; and
\item the network topology to determine the closest replica for every
  requesting \process.
\end{inparaenum}

\noindent A central server that registers both these
information~\cite{snamp, p2p-oracle, fogstore, p2p-alto} facilitates
the computation, for it gathers all knowledge in one place. However,
it comes with well-known issues inherent to its distribution paradigm
such as load balancing, or single point of failure.
%% Numerous solutions
%% maintain an index of all replica locations on a centralized
%% server. This comes with well-known disadvantages inherent to
%% centralized solutions such as load balancing, robustness or locality
%% that are even magnified in an Edge context.

\noindent Distributing this index among \processes circumvents these
issues~\cite{voronet, ipfs, mdht, squirrel, coin_19}, but still raises
locality issues where \processes
\begin{inparaenum}[(i)]
\item request to a possibly far away location service the content
  location, and then
\item request the actual content from far away replicas.
\end{inparaenum}
For instance, using \underline{d}istributed \underline{h}ash
\underline{t}ables (DHT)~\cite{ipfs, mdht, squirrel}, each \process
stores a part of the index defined by an interval between hash values.
The hash values are the keys to retrieve the associated content
location.  Before downloading any content, a \node asks for its
location using its key. After a series of round-trips between DHT
servers possibly distant from each others, the \node gets a list of
available replicas. Contrarily to \NAME, such services do not ensure
to include the closest replica.

\noindent In addition, they often do not include distance information
along with replica location. Determining where resides the closest
replica for every requesting \process necessarily involves some
knowledge about the \emph{current} topology.  Maintaining a consistent
view of an ever changing topology across a network is inherently
complicated~\cite{topology-discovery, ospf}.
%%  As for the index, this knowledge about the topology can be either
%% centralized~\cite{topology-discovery} or distributed like in
%% routing protocols such as \underline{O}pen \underline{S}hortest
%% \underline{P}ath \underline{F}irst (OSPF)~\cite{ospf}.
As consequence, the requesting \node ends up downloading from multiple
replica hosts, yet only keeping the fastest answer. \Processes either
waste network resources, or face lower response time.

% We emphasize that, in this setting, the DHT only stores information
% about replicas locations, not the content itself.

%% Distributing this index among \processes in the network circumvent
%% some of these issues, either using coordinate-based
%% overlays~\cite{voronet, coin_19} or \underline{D}istributed
%% \underline{H}ash \underline{T}ables (DHTs)~\cite{ipfs, mdht,
%%   squirrel}. In the latter, each node stores a part of the index,
%% defined by an interval between hash values.  Before downloading any
%% object, a node first hashes the content to obtain the address of the
%% node that stores the replica locations of this given object. After
%% obtaining from the remote node the list of available replicas, it can
%% select the closest copy to download from. We emphasize that, in this
%% setting, the DHT only stores information about replicas locations, not
%% the content itself.



%% \noindent All the aforementionned solutions require to contact a remote node to
%% request this index which is costly in terms of latency while in
%% contradiction with Edge infrastructure constraints, as previously
%% explained.


\item [Broadcast:]
The opposite approach consists in hosting the index directly on the
nodes, avoiding any prior request to access a content.
\underline{N}amed \underline{D}ata \underline{N}etworking
(NDN)~\cite{nlsr} broadcasts information about cache updates to all nodes in the
system. Having the entire knowledge of all the
replica locations along with distance information carried into
messages, each node can easily determine where the closest copy of a
given object resides, without contacting any remote node.  Similarly,
one could use a
\underline{C}onflict-\underline{F}ree \underline{R}eplicated
\underline{D}atatype (CRDT) for set data structures, for
example~\cite{shapiro2011crdts}. Yet both solutions inherently imply
that every node will eventually receive all messages, contrary to \NAME.


\item [Membership protocols:]
At first glance, membership protocols~\cite{t-man}
would appear to provide suitable abstractions to create a
scope-related partitioning, as proposed in this paper. Such protocols
usually rely on gossip mechanisms, implying contacts with random nodes
that can be far away in the physical topology. 
They are mostly implemented by modifying the
neighboring of a given node according to a certain metric in a logical
overlay. On the contrary,  \NAME  does not aim at building any overlay, as
it only relies on the physical topology so the neighbors of a given node
are not continuously modified. These protocols are usually
cycle-based, meaning that they impose a constant load on the network
traffic, even when the system state does not change. \NAME only generates traffic upon system changes.

\item [Timeouts:]
Solutions that periodically advertise their
cache content~\cite{garcia-lopez, hemmati2015namebased} also
incur an overhead even in quiet contexts without any update.
Moreover, they necessarily rely on physical timeouts that
could lead to either premature removals of partitions when they are
still operating; or slow convergence where processes wrongly believe
they belong to a partition that was removed. On the opposite,
Section~\ref{sec:experimentation} shows that \NAME quickly converges
to consistent partitioning even in large scale networks.

\end{asparadesc}

%Finally, we emphasize that consistency is at the core of our design,
%as \NAME provides the same guarantees for all nodes.  On the
%contrary, to avoid flooding, authors in~\cite{opnl} propose to
%propagate information about cache updates only towards the original
%replica. This enables for a request to be caught \textit{on-path},
%and be redirected to the closest replica. However, the announcement
%is only performed towards this original replica, that is requests can
%miss closer replicas that are not on this path, thus decreasing the
%efficiency of the cache sharing mechanism.

%AVK: This should go where we explain the DEL, as this is really
%specific...  One could also solve this issue by removing partitions
%that were not advertised for a defined
%time~\cite{hemmati2015namebased, garcia-lopez}.  However, relying on
%physical timeout could lead to either premature removals of
%partitions when they are still operating; or slow convergence where
%processes believe they belong to a partition that was removed
%(\TODO{plot ?}). In addition, since timeouts imply a continuous
%maintenance of partitions, such partitioning protocol incurs an
%overhead even in quiet contexts without dynamic partitions.


% AVK t-man: T-Man: Gossip-Based Overlay Topology Management5doat a
% random time at a random time once in eachconsecutive interval of T
% time unit This buffer contains a random sample of the nodes from the
% entirenetwork. It is provided by apeer sampling service We do not
% try to to create a topology aware overlay ! Because locality.  We do
% not provide routing ! routing is two determine path between any pair
% of nodes. We only provide path to objects Koala locality aware
% routing. Long links. Lazzy (nothing happens) Koala shares many core
% aspectswith similar overlays, such as Chord and Symphony.  VoroNet:
% overlay and routing. i.e. long links It links application objects
% rathen than physical nodes so that objects with similar
% characteristic are neighbors.  are logigal overlay, that are not
% correlated to the physical infrstructure.  Usually rely on long
% links...  AVK

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% ispell-local-dictionary: "english"
%%% End: 

